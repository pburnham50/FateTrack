{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"9bYiNy6aQ1dz"},"source":["# import packages\n","import pandas as pd\n","import numpy as np\n","import os\n","import sys\n","import dill"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9_YbGgmyNcO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633553903691,"user_tz":240,"elapsed":90,"user":{"displayName":"Yael Heyman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08333984778238961906"}},"outputId":"b8f4c8a5-53c9-4320-dc36-5c10583b4502"},"source":["# 1) properties\n","main = '/content/drive/MyDrive/FateTrack/'\n","#sample = '516_D4_2_w2_training'\n","sampleList = ['507_D2_3_w2_training', '516_D4_2_w2_training', '516_D2_1_w2_training']\n","agg_location = main + 'agg/'\n","annotation_location = main + 'agg/'\n","#blocks = 9\n","blockList = [16, 9, 7]\n","featureCols = [\"area\", \"filled_area\", \"bbox_area\", \"eccentricity\", \"solidity\", \"convex_area\",\n","            \"orientation\", \"major_axis_length\",\"minor_axis_length\", \"euler_number\", \"perimeter\", \"extent\",\n","            \"equivalent_diameter\", \"feret_diameter_max\", \"perimeter_crofton\",\n","            \"moments_central-0-0\", \"moments_central-0-1\", \"moments_central-0-2\", \"moments_central-0-3\",\n","            \"moments_central-1-0\", \"moments_central-1-1\", \"moments_central-1-2\", \"moments_central-1-3\",\n","            \"moments_central-2-0\", \"moments_central-2-1\", \"moments_central-2-2\", \"moments_central-2-3\",\n","            \"moments_central-3-0\", \"moments_central-3-1\", \"moments_central-3-2\", \"moments_central-3-3\",\n","            \"moments_hu-0\", \"moments_hu-1\", \"moments_hu-2\", \"moments_hu-3\", \"moments_hu-4\", \"moments_hu-5\", \"moments_hu-6\",\n","            \"moments-0-0\", \"moments-0-1\", \"moments-0-2\", \"moments-0-3\",\n","            \"moments-1-0\", \"moments-1-1\", \"moments-1-2\", \"moments-1-3\",\n","            \"moments-2-0\", \"moments-2-1\", \"moments-2-2\", \"moments-2-3\",\n","            \"moments-3-0\", \"moments-3-1\", \"moments-3-2\", \"moments-3-3\",\n","            \"inertia_tensor-0-0\", \"inertia_tensor-0-1\", \"inertia_tensor-1-0\", \"inertia_tensor-1-1\", \"inertia_tensor_eigvals-0\", \"inertia_tensor_eigvals-1\"]\n","nearbyFeatures = [\"area\", \"filled_area\", \"bbox_area\", \"eccentricity\", \"solidity\", \"convex_area\",\n","            \"orientation\", \"major_axis_length\",\"minor_axis_length\", \"perimeter\", \"extent\",\n","            \"equivalent_diameter\", \"feret_diameter_max\", \"perimeter_crofton\"]\n","nearbyFeaturesExtra = [\"nearby_cells\", \"mindist\"]\n","timelapseFeatures = []#[\"area\", \"eccentricity\", \"extent\", \"nearby_cells\"]\n","timelapseFeaturesExtra = []#[\"distance\"]\n","timepoints = [8]\n","channels = ['2-cy3', '2-yfp', '2-cy5', '1-yfp', '1-cy5', 'latent']\n","date = '0716' #change to current date"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['area', 'filled_area', 'bbox_area', 'eccentricity', 'solidity', 'convex_area', 'orientation', 'major_axis_length', 'minor_axis_length', 'perimeter', 'extent', 'equivalent_diameter', 'feret_diameter_max', 'perimeter_crofton']\n"]}]},{"cell_type":"code","metadata":{"id":"uMQiDF983nkI","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1633554644186,"user_tz":240,"elapsed":129,"user":{"displayName":"Yael Heyman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08333984778238961906"}},"outputId":"3d654f92-294e-4413-e400-fafaac83c8f4"},"source":["# combine measurement files\n","allMeasures = pd.DataFrame()\n","for block in range(blocks):\n","    allMeasures = pd.concat([allMeasures, pd.read_csv(agg_location + sample + '_block' + str(block) + 'of' + str(blocks) + '_output_features.csv')])\n","    print(all )\n","allMeasures.to_csv(agg_location + sample + '_measures_allblocks.csv')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-3d630011ca68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# combine measurement files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mallMeasures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mallMeasures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mallMeasures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_location\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_block'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'of'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_output_features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(allMeasures)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'blocks' is not defined"]}]},{"cell_type":"code","metadata":{"id":"tY1eZ0KeYuCx"},"source":["# 2a) load pickle object\n","def load_object(filename):\n","    with open(filename, 'rb') as red:\n","        tmp = dill.load(red)\n","    return(tmp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gt5SxB_DMHPR"},"source":["# 2b) functions to extract the nuclei of interest from the annotation file by block\n","def getNucleiOfInterest(block, blocks, sample):\n","    # load files\n","    labels = pd.read_csv(agg_location + sample + '_block' + str(block) + 'of' + str(blocks) + '_IDs.csv')\n","    features = pd.read_csv(agg_location + sample + '_block' + str(block) + 'of' + str(blocks) + '_input_features.csv')\n","    measures = pd.read_csv(agg_location + sample + '_block' + str(block) + 'of' + str(blocks) + '_output_features.csv')\n","    annotations = pd.read_csv(annotation_location + sample + '_block' + str(block) + 'of' + str(blocks) + '_annotations.csv')\n","    pklFile = load_object(main + sample + '_block' + str(block) + 'of' + str(blocks) + '.pkl')\n","    mask = pklFile.timelapse_nuclear_masks[8]\n","    # for i in range(features.shape[1]):\n","    #     print(features.columns[i])\n","    \n","    # filter out the cells on the border (features are inaccurate)\n","    initialLabels = annotations[annotations[\"annotation\"] != \"none\"]\n","    initialLabels = initialLabels[initialLabels[\"yCoord\"] > 50]\n","    initialLabels = initialLabels[initialLabels[\"xCoord\"] > 50]\n","    initialLabels = initialLabels[initialLabels[\"yCoord\"] < mask.shape[0] - 50]\n","    initialLabels = initialLabels[initialLabels[\"xCoord\"] < mask.shape[1] - 50]\n","\n","    # get labels of interest (Rd1_orig_label) from annotation file\n","    labels_Rd1 = np.asarray(initialLabels[\"pointID\"])\n","\n","    # remove duplicates (because of division)\n","    duplicates = list()\n","    for index, item in enumerate(sorted(np.asarray(labels[\"MasterID_8\"]))):\n","        if index == 0:\n","            prev = item\n","        elif prev == item:\n","            duplicates.append(item) \n","        else:\n","            prev = item\n","    labels_nodup = labels.loc[~labels[\"MasterID_8\"].isin(duplicates)]\n","    labels_filtered = labels_nodup.loc[labels_nodup[\"Rd1_orig_label\"].isin(labels_Rd1)]\n","\n","    # concatenate the labels of interest to be in full string format\n","    time_label_list = list()\n","    for i in range(labels_filtered.shape[0]):\n","        label_string = \"\"\n","        for t in range(9):\n","            col = \"MasterID_\" + str(t)\n","            label_string = label_string + str(labels_filtered.iloc[i].loc[col])\n","            if t < 8:\n","                label_string = label_string + \":\"\n","        time_label_list.append(label_string)\n","    HCR_label_list = labels_filtered[\"Rd1_orig_label\"]\n","\n","    # get features based on timelapse labels\n","    features_filtered = features[features[\"time8_label\"].isin(time_label_list)]\n","\n","    # get measures based on Rd1 labels\n","    measures_filtered = measures[measures[\"Rd1_orig_label\"].isin(HCR_label_list)]\n","\n","    # get ids based on non-duplicated labels\n","    ids = pd.DataFrame(annotations[annotations[\"pointID\"].isin(HCR_label_list)][\"annotation\"])\n","\n","    # append an identifier column\n","    identifiers = list()\n","    for i in range(labels_filtered.shape[0]):\n","        identifier = sample + \"_\" + str(block) + \"_\" + str(i)\n","        identifiers.append(identifier)\n","    identifiers = np.asarray(identifiers)\n","    features_filtered[\"identifier\"] = identifiers\n","    measures_filtered[\"identifier\"] = identifiers\n","    ids[\"identifier\"] = identifiers\n","\n","    # save files\n","    save_path = main + '/' + sample + '/'\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","    features_filtered.to_csv(save_path + 'block' + str(block) + '_features.csv')\n","    measures_filtered.to_csv(save_path + 'block' + str(block) + '_measures.csv')\n","    ids.to_csv(save_path + 'block' + str(block) + '_id.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQE56w1c9g_O"},"source":["# 3a) function to get names of features\n","def getFeatures(featureList):\n","    allFeatures = list()\n","    featureList.extend(nearbyFeaturesExtra)\n","    featureList.extend(timelapseFeaturesExtra)\n","    for time in timepoints:\n","        for feat in featureList:\n","            featName = \"time\" + str(time) + \"_\" + feat\n","            allFeatures.append(featName)\n","        for feat in nearbyFeatures:\n","            featNameMax = \"time\" + str(time) + \"_nearby_\" + feat + '_max'\n","            featNameMin = \"time\" + str(time) + \"_nearby_\" + feat + '_min'\n","            allFeatures.append(featNameMax)\n","            allFeatures.append(featNameMin)\n","        for feat in timelapseFeatures:\n","            featName = \"time\" + str(time) + \"_\" + feat + \"_dt\"\n","            allFeatures.append(featName)\n","    return allFeatures"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"74DqfxM_8_YN"},"source":["# 3b) function to normalize intensity measurements (group into quantiles)\n","def normMeas(meas_df):\n","  for i in range(meas_df.shape[1]):\n","    q_range = np.arange(0.1, 1.1, 0.1)\n","    q = np.quantile(meas_df.iloc[:,i], q_range)\n","    new_val = 1\n","    for j in range(meas_df.shape[0]): \n","      for k in range(10):\n","        if meas_df.iloc[j,i] <= q[9-k]:\n","          new_val = q_range[9-k]\n","      meas_df.iloc[j,i] = new_val\n","  return meas_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0KEbObfbvzX"},"source":["# 3b_2) function to normalize intensity measurements (group into quantiles and assign quantile value)\n","def normMeas(meas_df):\n","  for i in range(meas_df.shape[1]):\n","    q_range = np.arange(0.04, 1.04, 0.04)\n","    q = np.quantile(meas_df.iloc[:,i], q_range)\n","    new_val = 1\n","    for j in range(meas_df.shape[0]): \n","      for k in range(25):\n","        if meas_df.iloc[j,i] <= q[24-k]:\n","          new_val = q_range[24-k]\n","      meas_df.iloc[j,i] = new_val\n","  return meas_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzAIXwm3dmzk"},"source":["# 3b_3) scale intensities to between 0 and 1\n","def normMeas(meas_df):\n","  for i in range(meas_df.shape[1]):\n","    min_ch = min(meas_df.iloc[:,i])\n","    max_ch = max(meas_df.iloc[:,i])\n","    meas_df.iloc[:,i] = (meas_df.iloc[:,i] - min_ch) / (max_ch - min_ch)\n","  return meas_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GVhT4iCB6rbc"},"source":["# 3b_4) return raw intensities\n","def normMeas(meas_df):\n","    return meas_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxLPzGgp8ziO"},"source":["# 3c) function to get channels of interest\n","def processMeasures(nucMeas, channelList):\n","  filteredMeas = pd.DataFrame()\n","  for channel in channelList:\n","    if channel == '1-cy5':\n","      ch = 'Rd1_CY5'\n","    elif channel == '1-yfp':\n","      ch = 'Rd1_YFP'\n","    elif channel == '2-cy3':\n","      ch = 'Rd2_CY3'\n","    elif channel == '2-cy5':\n","      ch = 'Rd2_CY5'\n","    elif channel == '2-yfp':\n","      ch = 'Rd2_YFP'\n","    else:\n","        continue\n","    mean_ch_name = ch + '_mean_intensity'\n","    # ### comment out later\n","    # q0 = ch + '_quantiles-0'\n","    # q1 = ch + '_quantiles-1'\n","    # q2 = ch + '_quantiles-2'\n","    # q3 = ch + '_quantiles-3'\n","    # q4 = ch + '_quantiles-4'\n","    # colList = [mean_ch_name, q0, q1, q2, q3, q4]\n","    # filteredMeas = pd.concat([filteredMeas, pd.DataFrame(nucMeas[colList])])\n","    filteredMeas = pd.concat([filteredMeas, pd.DataFrame(nucMeas[mean_ch_name])], axis = 1)\n","  return filteredMeas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"80WOad2PGHkQ"},"source":["# 4) function to aggregate data from blocks and extract for ML\n","def prepareData(prefix, sampleList, blockList, all = False):\n","    allIDs = pd.DataFrame()\n","    allFeats = pd.DataFrame()\n","    allMeas = pd.DataFrame()\n","    allSampleNames = pd.DataFrame()\n","\n","    i = 0\n","    for sample in sampleList:\n","        # aggregate features and measures\n","        features = pd.DataFrame()\n","        measures = pd.DataFrame()\n","        ids = pd.DataFrame()\n","        for b in range(blockList[i]):\n","            block_features = pd.read_csv(main + sample + '/' + 'block' + str(b) + '_features.csv')\n","            block_measures = pd.read_csv(main + sample + '/' + 'block' + str(b) + '_measures.csv')\n","            block_ids = pd.read_csv(main + sample + '/' + 'block' + str(b) + '_id.csv')\n","            features = features.append(block_features)\n","            measures = measures.append(block_measures)\n","            ids = ids.append(block_ids)\n","        unnormMeasures = pd.concat([pd.DataFrame(measures[\"identifier\"]), processMeasures(measures, channels)], axis = 1)\n","        measures = pd.concat([pd.DataFrame(measures[\"identifier\"]), normMeas(processMeasures(measures, channels))], axis = 1)\n","        sampleCol = pd.DataFrame([sample] * ids.shape[0])\n","        sampleNamesIDs = (pd.DataFrame(ids[\"identifier\"])).reset_index()\n","        sampleCol[\"identifier\"] = sampleNamesIDs[\"identifier\"]\n","        allIDs = pd.concat([allIDs, ids])\n","        allFeats = pd.concat([allFeats, features])\n","        allMeas = pd.concat([allMeas, measures])\n","        allSampleNames = pd.concat([allSampleNames, sampleCol])\n","        i += 1\n","\n","    # sample cells from channels\n","    if (all):\n","        sampledIDs = allIDs\n","        sampledFeats = allFeats\n","        sampledMeas = allMeas\n","        sampledSNames = allSampleNames\n","    else:\n","        minCells = sys.maxsize\n","        for channel in channels:\n","            chanIDs = allIDs[allIDs[\"annotation\"] == channel]\n","            nrow = chanIDs.shape[0]\n","            if nrow < minCells:\n","                minCells = nrow\n","        sampledIDs = pd.DataFrame()\n","        sampledFeats = pd.DataFrame()\n","        sampledMeas = pd.DataFrame()\n","        sampledSNames = pd.DataFrame()\n","        for channel in channels:\n","            chanIDs = allIDs[allIDs[\"annotation\"] == channel]\n","            if chanIDs.shape[0] > minCells:\n","                chanIDs = chanIDs.sample(minCells, replace = False)\n","            sampledIdentifiers = chanIDs[\"identifier\"]\n","            chanFeats = allFeats[allFeats[\"identifier\"].isin(sampledIdentifiers)]\n","            chanMeas = allMeas[allMeas[\"identifier\"].isin(sampledIdentifiers)]\n","            chanSNames = allSampleNames[allSampleNames[\"identifier\"].isin(sampledIdentifiers)]\n","            sampledIDs = pd.concat([sampledIDs, chanIDs])\n","            sampledFeats = pd.concat([sampledFeats, chanFeats])\n","            sampledMeas = pd.concat([sampledMeas, chanMeas])\n","            sampledSNames = pd.concat([sampledSNames, chanSNames])\n","\n","    # get the important columns\n","    sampledIDs = sampledIDs[\"annotation\"]\n","    featsOfInterest = getFeatures(featureCols)\n","    sampledFeats = sampledFeats[featsOfInterest]\n","    sampledMeas = sampledMeas.drop(labels = \"identifier\", axis = 1)\n","    sampledSNames = sampledSNames.drop(labels = \"identifier\", axis = 1)\n","    unnormMeasures = unnormMeasures.drop(labels = \"identifier\", axis = 1)\n","\n","    # save files\n","    channelString = \"\"\n","    for channel in channels:\n","        channelString = channelString + \"_\" + channel\n","        channelString.replace(\"-\", \"\")\n","    save_path = main + '/' + date + '/'\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","    featsOfInterest = pd.DataFrame(featsOfInterest)\n","    featsOfInterest.to_csv(save_path + 'features' + date + prefix + channelString + '.csv')\n","    sampledIDs.to_csv(save_path + 'nuclei_id_' + date + prefix + channelString + '.csv', header = False, index=False)\n","    sampledFeats.to_csv(save_path + 'nuclei_feat_' + date + prefix + channelString + '.csv', header = False, index=False)\n","    sampledMeas.to_csv(save_path + 'nuclei_meas_' + date + prefix + channelString + '.csv', header = False, index=False)\n","    sampledSNames.to_csv(save_path + 'nuclei_samples_' + date + prefix + channelString + '.csv', header = False, index = False)\n","    # unnormMeasures.to_csv(save_path + 'unnorm_meas' + date + prefix + channelString + '.csv', header = False, index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Y3ksy_VJNMc"},"source":["# 5) run\n","i = 0\n","for s in sampleList:\n","    totalBlocks = blockList[i]\n","    for b in range(totalBlocks):\n","        getNucleiOfInterest(b, totalBlocks, s)\n","    i += 1\n","s = sampleList[2]\n","totalBlocks = 9\n","for b in range(totalBlocks):\n","    getNucleiOfInterest(b, totalBlocks, s)\n","\n","prefix = '_allNuc_q25' # comes after data but before channel names\n","prepareData(prefix, sampleList, blockList, all = True)\n"],"execution_count":null,"outputs":[]}]}