{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wqlN6SsVSUyb"},"source":["run convolutional neural network for multi-classification or multi-output regression, input=images\n","\n"]},{"cell_type":"code","metadata":{"id":"DLBfl6A8SSbL"},"source":["# import various libraries\n","import numpy as np\n","import csv\n","import pandas as pd\n","import argparse\n","import sklearn\n","import keras\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RepeatedKFold\n","from sklearn.preprocessing import OneHotEncoder\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, BatchNormalization\n","from keras import backend\n","from pathlib import Path\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CpqmqkMOSbcT"},"source":["# parse arguments\n","dir = '/content/drive/MyDrive/FateTrack/0602/HCR_507_D2_3_w2/collections_2021616/' #directory where training/test data is saved\n","suff = 'Clu_epithelial' #data suffix (usually date)\n","out = '0621_Clu_C_kernel' #output suffix (usually same as suff)\n","n_epochs = 60 #number of epochs for training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1aufzfobSIcC"},"source":["# get the dataset\n","def get_dataset(): #change this to load in data\n","    #X_train = pd.read_csv(dir + \"train_\" + suff + \".csv\", sep = ',', header = None)\n","    #y_train = pd.read_csv(dir + \"train_id_\" + suff + \".csv\", sep = ',', header = None)\n","    #X_test = pd.read_csv(dir + \"test_\" + suff + \".csv\", sep = ',', header = None)\n","    #y_test = pd.read_csv(dir + \"test_id_\" + suff + \".csv\", sep = ',', header = None)\n","\n","    # X_train = np.load(dir + \"nuclei\" + suff + \".npy\")\n","    # X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n","    # y_train = np.asarray(pd.read_csv(dir + \"nuclei_id\" + suff + \".csv\", header = None))\n","    # encoder = OneHotEncoder(sparse=False)\n","    # y_train = encoder.fit_transform(y_train)\n","    # X_test = pd.DataFrame()\n","    # y_test = pd.DataFrame()\n","    # #y_test = encoder.transform(y_test)\n","\n","    # train_test_split of full dataset\n","    X = np.load(dir + \"nuclei_\" + suff + \".npy\")\n","    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n","    #y = np.asarray(pd.read_csv(dir + \"nuclei_id_\" + suff + \".csv\", header = None))\n","    y = np.asarray(pd.read_csv(dir + \"nuclei_id_\" + suff + \".csv\", header = None))\n","    encoder = OneHotEncoder(sparse=False)\n","    #y = encoder.fit_transform(y)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 2, stratify = y) # use stratify for classification\n","    return X_train, X_test, y_train, y_test, encoder\n","\n","# get the model\n","def get_model(n_inputs, n_outputs):\n","    model = Sequential()\n","    model.add(Conv2D(20, kernel_size=5, activation='relu', input_shape=(60,60,1), padding = 'same')) #change input shape based on images\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(pool_size = (2,2)))\n","    #model.add(Dropout(0.25))\n","    model.add(Conv2D(60, kernel_size=4, activation='relu', padding = 'same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(pool_size = (2,2)))\n","    #model.add(Dropout(0.25))\n","    model.add(Conv2D(100, kernel_size=3, activation='relu', padding = 'same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(pool_size = (2,2)))\n","    #model.add(Dropout(0.25))\n","    model.add(Flatten()) #can also add another dense layer before output if you want\n","    model.add(Dense(500, activation = 'relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.5))\n","    model.add(Dense(50, activation = 'relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.5))\n","    #model.add(Dense(n_outputs, activation = 'sigmoid')) # binary classification\n","    #model.add(Dense(n_outputs, activation = 'softmax')) # multi-classification\n","    model.add(Dense(n_outputs, activation = 'relu')) # regression\n","    #model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['categorical_accuracy']) # binary-classification\n","    #model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['categorical_accuracy']) # multi-classification\n","    model.compile(loss = 'mse', optimizer = 'adam', metrics = ['mse']) # regression\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gWuEl0u8TGHz","executionInfo":{"status":"error","timestamp":1624302850305,"user_tz":240,"elapsed":287,"user":{"displayName":"Laila Norford","photoUrl":"","userId":"14900707631576672578"}},"outputId":"6197627d-e2db-4f6d-f762-6654dd366217"},"source":["# load dataset\n","X_train, X_test, y_train, y_test, encoder = get_dataset()\n","n_inputs, n_outputs = X_train.shape[0], y_train.shape[1]\n","X_train = tf.convert_to_tensor(X_train)\n","X_test = tf.convert_to_tensor(X_test)\n","y_train = tf.convert_to_tensor(y_train)\n","y_test = tf.convert_to_tensor(y_test)\n","\n","# get model\n","model = get_model(n_inputs, n_outputs)\n","model.summary()\n","# fit the model on all data\n","history = model.fit(X_train, y_train, validation_data = (X_test, y_test), verbose = 2, epochs = n_epochs)#, batch_size = 25)\n","# intermediate_layer_model = keras.Model(inputs=model.input,\n","#                                        outputs=model.layers[-2].output)\n","# intermediate_output = intermediate_layer_model(keras.Input([X_train, y_train]))\n","# print(intermediate_output)\n","\n","# # make a prediction for new data and write out to csv\n","y_pred = model.predict(X_test)\n","y_test_labels = encoder.inverse_transform(y_test)\n","y_pred_labels = encoder.inverse_transform(y_pred)\n","results = pd.DataFrame(np.hstack((y_pred_labels, y_test_labels)))\n","results.to_csv(dir + \"results_\" + out + \".csv\")\n","\n","# # # write accuracy and validation accuracy out to csv\n","# accuracy = pd.DataFrame(data=history.history['accuracy'])\n","# val_accuracy = pd.DataFrame(history.history['val_accuracy'])\n","# accuracy = pd.concat([accuracy, val_accuracy], axis=1)\n","# accuracy.to_csv(dir + 'accuracy_' + out + '.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_46\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_132 (Conv2D)          (None, 60, 60, 20)        520       \n","_________________________________________________________________\n","batch_normalization_70 (Batc (None, 60, 60, 20)        80        \n","_________________________________________________________________\n","max_pooling2d_127 (MaxPoolin (None, 30, 30, 20)        0         \n","_________________________________________________________________\n","conv2d_133 (Conv2D)          (None, 30, 30, 60)        19260     \n","_________________________________________________________________\n","batch_normalization_71 (Batc (None, 30, 30, 60)        240       \n","_________________________________________________________________\n","max_pooling2d_128 (MaxPoolin (None, 15, 15, 60)        0         \n","_________________________________________________________________\n","conv2d_134 (Conv2D)          (None, 15, 15, 100)       54100     \n","_________________________________________________________________\n","batch_normalization_72 (Batc (None, 15, 15, 100)       400       \n","_________________________________________________________________\n","max_pooling2d_129 (MaxPoolin (None, 7, 7, 100)         0         \n","_________________________________________________________________\n","flatten_45 (Flatten)         (None, 4900)              0         \n","_________________________________________________________________\n","dense_99 (Dense)             (None, 500)               2450500   \n","_________________________________________________________________\n","batch_normalization_73 (Batc (None, 500)               2000      \n","_________________________________________________________________\n","dropout_59 (Dropout)         (None, 500)               0         \n","_________________________________________________________________\n","dense_100 (Dense)            (None, 50)                25050     \n","_________________________________________________________________\n","batch_normalization_74 (Batc (None, 50)                200       \n","_________________________________________________________________\n","dropout_60 (Dropout)         (None, 50)                0         \n","_________________________________________________________________\n","dense_101 (Dense)            (None, 2)                 102       \n","=================================================================\n","Total params: 2,552,452\n","Trainable params: 2,550,992\n","Non-trainable params: 1,460\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-98-caf4dc4cb06d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m intermediate_layer_model = keras.Model(inputs=model.input,\n\u001b[1;32m     15\u001b[0m                                        outputs=model.layers[-2].output)\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, name, dtype, sparse, tensor, ragged, type_spec, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m     input_layer_config.update(\n\u001b[1;32m    382\u001b[0m         {'batch_size': batch_size, 'input_shape': shape})\n\u001b[0;32m--> 383\u001b[0;31m   \u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_layer_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m   \u001b[0;31m# Return tensor including `_keras_history`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, type_spec, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             ragged=ragged)\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(shape, ndim, dtype, sparse, name, ragged)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m       spec = tf.TensorSpec(\n\u001b[0;32m-> 1314\u001b[0;31m           shape=shape, dtype=dtype, name=name)\n\u001b[0m\u001b[1;32m   1315\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_tensor_from_type_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_spec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, dtype, name)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mnot\u001b[0m \u001b[0mconvertible\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \"\"\"\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \"\"\"\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# int(...) compensates for the int/long dichotomy on Python 2.X.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;31m# TODO(b/143206389): Remove once we fully migrate to 3.X.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__index__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         six.raise_from(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__index__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__index__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__index__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"]}]}]}