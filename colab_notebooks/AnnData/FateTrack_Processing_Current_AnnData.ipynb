{"cells":[{"cell_type":"markdown","metadata":{"id":"Bl7quVxEcurS"},"source":["# Here we show give an example of our full pipeline to extract data from timelapse."]},{"cell_type":"markdown","metadata":{"id":"LRTPbmRp6m5z"},"source":["### Load all relevant packages and connecting to your Drive\n","### * indicates not necessary to run pipeline\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19002,"status":"ok","timestamp":1669933112620,"user":{"displayName":"Fate Track","userId":"00899138229569203583"},"user_tz":300},"id":"fDykxXRZX9SC","outputId":"e6dcec63-07c6-4f75-e8c3-8b310133e2e2","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Your runtime has 89.6 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n","Thu Dec  1 22:18:31 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   23C    P0    43W / 400W |      0MiB / 40536MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["#@markdown #### Mount google drive and check that you are using high RAM and GPU resources.\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOfq2ouq4u0Z","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":857},"executionInfo":{"status":"ok","timestamp":1667832348821,"user_tz":300,"elapsed":8670,"user":{"displayName":"Fate Track","userId":"00899138229569203583"}},"outputId":"eb25e96c-310a-4e4b-dc8d-dac565e5be37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ipdb\n","  Downloading ipdb-0.13.9.tar.gz (16 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n","Collecting ipython>=7.17.0\n","  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n","\u001b[K     |████████████████████████████████| 793 kB 47.9 MB/s \n","\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n","Collecting matplotlib-inline\n","  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.0.10)\n","Collecting jedi>=0.16\n","  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 68.0 MB/s \n","\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (1.15.0)\n","Building wheels for collected packages: ipdb\n","  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=f80b6052d00f733a2b400866df300c8cae41c417ee430e5e50d7e06a03ea1381\n","  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n","Successfully built ipdb\n","Installing collected packages: matplotlib-inline, jedi, ipython, ipdb\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 7.9.0\n","    Uninstalling ipython-7.9.0:\n","      Successfully uninstalled ipython-7.9.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n","Successfully installed ipdb-0.13.9 ipython-7.34.0 jedi-0.18.1 matplotlib-inline-0.1.6\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython"]}}},"metadata":{}}],"source":["#@markdown ###*Install debugger\n","!pip install ipdb\n","import ipdb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-rH256mGr1g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667832355117,"user_tz":300,"elapsed":6305,"user":{"displayName":"Fate Track","userId":"00899138229569203583"}},"outputId":"79a91d24-7d8f-463e-9e18-57acdc2cd0da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting nd2reader\n","  Downloading nd2reader-3.3.0-py2.py3-none-any.whl (36 kB)\n","Collecting pims>=0.3.0\n","  Downloading PIMS-0.6.1.tar.gz (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from nd2reader) (1.21.6)\n","Collecting xmltodict>=0.9.2\n","  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n","Requirement already satisfied: six>=1.4 in /usr/local/lib/python3.7/dist-packages (from nd2reader) (1.15.0)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from pims>=0.3.0->nd2reader) (2.9.0)\n","Collecting slicerator>=0.9.8\n","  Downloading slicerator-1.1.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio->pims>=0.3.0->nd2reader) (7.1.2)\n","Building wheels for collected packages: pims\n","  Building wheel for pims (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pims: filename=PIMS-0.6.1-py3-none-any.whl size=82636 sha256=896f81a5007f22725ef976b463a42926b341b2a91c4377992a92b3137a554d5f\n","  Stored in directory: /root/.cache/pip/wheels/8e/d5/a9/f702433436655b7a2bc7ff93efd742587dd5abd44f7e406917\n","Successfully built pims\n","Installing collected packages: slicerator, xmltodict, pims, nd2reader\n","Successfully installed nd2reader-3.3.0 pims-0.6.1 slicerator-1.1.0 xmltodict-0.13.0\n"]}],"source":["!pip install nd2reader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrGMrSLScncV","cellView":"form"},"outputs":[],"source":["%%capture\n","from __future__ import print_function, unicode_literals, absolute_import, division\n","\n","Notebook_version = ['1.12']\n","\n","#@markdown ####Install packages related to CARE\n","\n","#Libraries contains information of certain topics. \n","#For example the tifffile library contains information on how to handle tif-files.\n","\n","#Here, we install libraries which are not already included in Colab.\n","\n","!pip install tifffile # contains tools to operate tiff-files\n","!pip install csbdeep  # contains tools for restoration of fluorescence microcopy images (Content-aware Image Restoration, CARE). It uses Keras and Tensorflow.\n","!pip install wget\n","!pip install memory_profiler\n","!pip install fpdf\n","!pip install scikit-image==0.18\n","#!pip install protobuf==3.20.1\n","#!pip install protoc==3.19.0\n","#!pip install tensorflow==2.9.1\n","%load_ext memory_profiler\n","\n","#Here, we import and enable Tensorflow 1 instead of Tensorflow 2.\n","#%tensorflow_version 1.x\n","\n","import sys\n","before = [str(m) for m in sys.modules]\n","\n","import tensorflow\n","import tensorflow as tf\n","\n","print(tensorflow.__version__)\n","print(\"Tensorflow enabled.\")\n","\n","# from __future__ import print_function, unicode_literals, absolute_import, division\n","\n","# ------- Variable specific to CARE -------\n","from csbdeep.utils import download_and_extract_zip_file, plot_some, axes_dict, plot_history, Path, download_and_extract_zip_file\n","from csbdeep.data import RawData, create_patches \n","from csbdeep.io import load_training_data, save_tiff_imagej_compatible\n","from csbdeep.models import Config, CARE\n","from csbdeep import data\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","\n","\n","# ------- Common variable to all ZeroCostDL4Mic notebooks -------\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import urllib\n","import os, random\n","import shutil \n","import zipfile\n","from tifffile import imread, imsave\n","import time\n","import sys\n","import wget\n","from pathlib import Path\n","import pandas as pd\n","import csv\n","from glob import glob\n","from scipy import signal\n","from scipy import ndimage\n","from skimage import io\n","from sklearn.linear_model import LinearRegression\n","from skimage.util import img_as_uint\n","import matplotlib as mpl\n","from skimage.metrics import structural_similarity\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from astropy.visualization import simple_norm\n","from skimage import img_as_float32\n","from skimage.util import img_as_ubyte\n","from tqdm import tqdm \n","from fpdf import FPDF, HTMLMixin\n","from datetime import datetime\n","import subprocess\n","from pip._internal.operations.freeze import freeze\n","\n","# Colors for the warning messages\n","class bcolors:\n","  WARNING = '\\033[31m'\n","\n","W  = '\\033[0m'  # white (normal)\n","R  = '\\033[31m' # red\n","\n","#Disable some of the tensorflow warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","print(\"Libraries installed\")\n","\n","\n","# Check if this is the latest version of the notebook\n","Latest_notebook_version = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_ZeroCostDL4Mic_Release.csv\")\n","\n","if Notebook_version == list(Latest_notebook_version.columns):\n","  print(\"This notebook is up-to-date.\")\n","\n","if not Notebook_version == list(Latest_notebook_version.columns):\n","  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n","\n","!pip freeze > requirements.txt\n","\n","#Create a pdf document with training summary\n","def pdf_export(trained = False, augmentation = False, pretrained_model = False):\n","    \"\"\"Create a pdf document with training summary for CARE.\"\"\"\n","    \n","    # save FPDF() class into a  \n","    # variable pdf \n","    #from datetime import datetime\n","\n","    class MyFPDF(FPDF, HTMLMixin):\n","        pass\n","\n","    pdf = MyFPDF()\n","    pdf.add_page()\n","    pdf.set_right_margin(-1)\n","    pdf.set_font(\"Arial\", size = 11, style='B') \n","\n","    Network = 'CARE 2D'\n","    day = datetime.now()\n","    datetime_str = str(day)[0:10]\n","\n","    Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n","    pdf.multi_cell(180, 5, txt = Header, align = 'L') \n","\n","    # add another cell \n","    if trained:\n","      training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n","      pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n","    pdf.ln(1)\n","\n","    Header_2 = 'Information for your materials and methods:'\n","    pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n","\n","    all_packages = ''\n","    for requirement in freeze(local_only=True):\n","      all_packages = all_packages+requirement+', '\n","    #print(all_packages)\n","\n","    #Main Packages\n","    main_packages = ''\n","    version_numbers = []\n","    for name in ['tensorflow','numpy','Keras','csbdeep']:\n","      find_name=all_packages.find(name)\n","      main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n","      #Version numbers only here:\n","      version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n","\n","    cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n","    cuda_version = cuda_version.stdout.decode('utf-8')\n","    cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n","    gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n","    gpu_name = gpu_name.stdout.decode('utf-8')\n","    gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n","    #print(cuda_version[cuda_version.find(', V')+3:-1])\n","    #print(gpu_name)\n","\n","    shape = io.imread(Training_source+'/'+os.listdir(Training_source)[1]).shape\n","    dataset_size = len(os.listdir(Training_source))\n","\n","    text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(dataset_size*number_of_patches)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_size)+','+str(patch_size)+')) with a batch size of '+str(batch_size)+' and a '+config.train_loss+' loss function, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), csbdeep (v '+version_numbers[3]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n","\n","    if pretrained_model:\n","      text = 'The '+Network+' model was trained for '+str(number_of_epochs)+' epochs on '+str(dataset_size*number_of_patches)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_size)+','+str(patch_size)+')) with a batch size of '+str(batch_size)+' and a '+config.train_loss+' loss function, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). The model was re-trained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), csbdeep (v '+version_numbers[3]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n","\n","    pdf.set_font('')\n","    pdf.set_font_size(10.)\n","    pdf.multi_cell(190, 5, txt = text, align='L')\n","    pdf.set_font('')\n","    pdf.set_font('Arial', size = 10, style = 'B')\n","    pdf.ln(1)\n","    pdf.cell(28, 5, txt='Augmentation: ', ln=0)\n","    pdf.set_font('')\n","    if augmentation:\n","      aug_text = 'The dataset was augmented by a factor of '+str(Multiply_dataset_by)+' by'\n","      if rotate_270_degrees != 0 or rotate_90_degrees != 0:\n","        aug_text = aug_text+'\\n- rotation'\n","      if flip_left_right != 0 or flip_top_bottom != 0:\n","        aug_text = aug_text+'\\n- flipping'\n","      if random_zoom_magnification != 0:\n","        aug_text = aug_text+'\\n- random zoom magnification'\n","      if random_distortion != 0:\n","        aug_text = aug_text+'\\n- random distortion'\n","      if image_shear != 0:\n","        aug_text = aug_text+'\\n- image shearing'\n","      if skew_image != 0:\n","        aug_text = aug_text+'\\n- image skewing'\n","    else:\n","      aug_text = 'No augmentation was used for training.'\n","    pdf.multi_cell(190, 5, txt=aug_text, align='L')\n","    pdf.set_font('Arial', size = 11, style = 'B')\n","    pdf.ln(1)\n","    pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n","    pdf.set_font('')\n","    pdf.set_font_size(10.)\n","    if Use_Default_Advanced_Parameters:\n","      pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n","    pdf.cell(200, 5, txt='The following parameters were used for training:')\n","    pdf.ln(1)\n","    html = \"\"\" \n","    <table width=40% style=\"margin-left:0px;\">\n","      <tr>\n","        <th width = 50% align=\"left\">Parameter</th>\n","        <th width = 50% align=\"left\">Value</th>\n","      </tr>\n","      <tr>\n","        <td width = 50%>number_of_epochs</td>\n","        <td width = 50%>{0}</td>\n","      </tr>\n","      <tr>\n","        <td width = 50%>patch_size</td>\n","        <td width = 50%>{1}</td>\n","      </tr>\n","      <tr>\n","        <td width = 50%>number_of_patches</td>\n","        <td width = 50%>{2}</td>\n","      </tr>\n","      <tr>\n","        <td width = 50%>batch_size</td>\n","        <td width = 50%>{3}</td>\n","      </tr>\n","      <tr>\n","        <td width = 50%>number_of_steps</td>\n","        <td width = 50%>{4}</td>\n","      </tr>\n","      <tr>\n","        <td width = 50%>percentage_validation</td>\n","        <td width = 50%>{5}</td>\n","      </tr>\n","      <tr>\n","        <td width = 50%>initial_learning_rate</td>\n","        <td width = 50%>{6}</td>\n","      </tr>\n","    </table>\n","    \"\"\".format(number_of_epochs,str(patch_size)+'x'+str(patch_size),number_of_patches,batch_size,number_of_steps,percentage_validation,initial_learning_rate)\n","    pdf.write_html(html)\n","\n","    #pdf.multi_cell(190, 5, txt = text_2, align='L')\n","    pdf.set_font(\"Arial\", size = 11, style='B')\n","    pdf.ln(1)\n","    pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n","    pdf.set_font('')\n","    pdf.set_font('Arial', size = 10, style = 'B')\n","    pdf.cell(29, 5, txt= 'Training_source:', align = 'L', ln=0)\n","    pdf.set_font('')\n","    pdf.multi_cell(170, 5, txt = Training_source, align = 'L')\n","    pdf.set_font('')\n","    pdf.set_font('Arial', size = 10, style = 'B')\n","    pdf.cell(27, 5, txt= 'Training_target:', align = 'L', ln=0)\n","    pdf.set_font('')\n","    pdf.multi_cell(170, 5, txt = Training_target, align = 'L')\n","    #pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n","    pdf.ln(1)\n","    pdf.set_font('')\n","    pdf.set_font('Arial', size = 10, style = 'B')\n","    pdf.cell(22, 5, txt= 'Model Path:', align = 'L', ln=0)\n","    pdf.set_font('')\n","    pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n","    pdf.ln(1)\n","    pdf.cell(60, 5, txt = 'Example Training pair', ln=1)\n","    pdf.ln(1)\n","    exp_size = io.imread('/content/TrainingDataExample_CARE2D.png').shape\n","    pdf.image('/content/TrainingDataExample_CARE2D.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n","    pdf.ln(1)\n","    ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy.\" BioRxiv (2020).'\n","    pdf.multi_cell(190, 5, txt = ref_1, align='L')\n","    ref_2 = '- CARE: Weigert, Martin, et al. \"Content-aware image restoration: pushing the limits of fluorescence microscopy.\" Nature methods 15.12 (2018): 1090-1097.'\n","    pdf.multi_cell(190, 5, txt = ref_2, align='L')\n","    if augmentation:\n","      ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n","      pdf.multi_cell(190, 5, txt = ref_3, align='L')\n","    pdf.ln(3)\n","    reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n","    pdf.set_font('Arial', size = 11, style='B')\n","    pdf.multi_cell(190, 5, txt=reminder, align='C')\n","\n","    pdf.output(model_path+'/'+model_name+'/'+model_name+\"_training_report.pdf\")\n","\n","\n","#Make a pdf summary of the QC results\n","\n","def qc_pdf_export():\n","  \"\"\"Make a PDF summary of the quality control results from CARE.\"\"\"\n","  class MyFPDF(FPDF, HTMLMixin):\n","    pass\n","\n","  pdf = MyFPDF()\n","  pdf.add_page()\n","  pdf.set_right_margin(-1)\n","  pdf.set_font(\"Arial\", size = 11, style='B') \n","\n","  Network = 'CARE 2D'\n","  #model_name = os.path.basename(full_QC_model_path)\n","  day = datetime.now()\n","  datetime_str = str(day)[0:10]\n","\n","  Header = 'Quality Control report for '+Network+' model ('+QC_model_name+')\\nDate: '+datetime_str\n","  pdf.multi_cell(180, 5, txt = Header, align = 'L') \n","\n","  all_packages = ''\n","  for requirement in freeze(local_only=True):\n","    all_packages = all_packages+requirement+', '\n","\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 11, style = 'B')\n","  pdf.ln(2)\n","  pdf.cell(190, 5, txt = 'Development of Training Losses', ln=1, align='L')\n","  pdf.ln(1)\n","  exp_size = io.imread(full_QC_model_path+'Quality Control/QC_example_data.png').shape\n","  if os.path.exists(full_QC_model_path+'Quality Control/lossCurvePlots.png'):\n","    pdf.image(full_QC_model_path+'Quality Control/lossCurvePlots.png', x = 11, y = None, w = round(exp_size[1]/10), h = round(exp_size[0]/13))\n","  else:\n","    pdf.set_font('')\n","    pdf.set_font('Arial', size=10)\n","    pdf.multi_cell(190, 5, txt='If you would like to see the evolution of the loss function during training please play the first cell of the QC section in the notebook.', align='L')\n","  pdf.ln(2)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.ln(3)\n","  pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n","  pdf.ln(1)\n","  exp_size = io.imread(full_QC_model_path+'Quality Control/QC_example_data.png').shape\n","  pdf.image(full_QC_model_path+'Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/10), h = round(exp_size[0]/10))\n","  pdf.ln(1)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 11, style = 'B')\n","  pdf.ln(1)\n","  pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n","  pdf.set_font('')\n","  pdf.set_font_size(10.)\n","\n","  pdf.ln(1)\n","  html = \"\"\"\n","  <body>\n","  <font size=\"7\" face=\"Courier New\" >\n","  <table width=94% style=\"margin-left:0px;\">\"\"\"\n","  with open(full_QC_model_path+'Quality Control/QC_metrics_'+QC_model_name+'.csv', 'r') as csvfile:\n","    metrics = csv.reader(csvfile)\n","    header = next(metrics)\n","    image = header[0]\n","    mSSIM_PvsGT = header[1]\n","    mSSIM_SvsGT = header[2]\n","    NRMSE_PvsGT = header[3]\n","    NRMSE_SvsGT = header[4]\n","    PSNR_PvsGT = header[5]\n","    PSNR_SvsGT = header[6]\n","    header = \"\"\"\n","    <tr>\n","      <th width = 10% align=\"left\">{0}</th>\n","      <th width = 15% align=\"left\">{1}</th>\n","      <th width = 15% align=\"center\">{2}</th>\n","      <th width = 15% align=\"left\">{3}</th>\n","      <th width = 15% align=\"center\">{4}</th>\n","      <th width = 15% align=\"left\">{5}</th>\n","      <th width = 15% align=\"center\">{6}</th>\n","    </tr>\"\"\".format(image,mSSIM_PvsGT,mSSIM_SvsGT,NRMSE_PvsGT,NRMSE_SvsGT,PSNR_PvsGT,PSNR_SvsGT)\n","    html = html+header\n","    for row in metrics:\n","      image = row[0]\n","      mSSIM_PvsGT = row[1]\n","      mSSIM_SvsGT = row[2]\n","      NRMSE_PvsGT = row[3]\n","      NRMSE_SvsGT = row[4]\n","      PSNR_PvsGT = row[5]\n","      PSNR_SvsGT = row[6]\n","      cells = \"\"\"\n","        <tr>\n","          <td width = 10% align=\"left\">{0}</td>\n","          <td width = 15% align=\"center\">{1}</td>\n","          <td width = 15% align=\"center\">{2}</td>\n","          <td width = 15% align=\"center\">{3}</td>\n","          <td width = 15% align=\"center\">{4}</td>\n","          <td width = 15% align=\"center\">{5}</td>\n","          <td width = 15% align=\"center\">{6}</td>\n","        </tr>\"\"\".format(image,str(round(float(mSSIM_PvsGT),3)),str(round(float(mSSIM_SvsGT),3)),str(round(float(NRMSE_PvsGT),3)),str(round(float(NRMSE_SvsGT),3)),str(round(float(PSNR_PvsGT),3)),str(round(float(PSNR_SvsGT),3)))\n","      html = html+cells\n","    html = html+\"\"\"</body></table>\"\"\"\n","\n","  pdf.write_html(html)\n","\n","  pdf.ln(1)\n","  pdf.set_font('')\n","  pdf.set_font_size(10.)\n","  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy.\" BioRxiv (2020).'\n","  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n","  ref_2 = '- CARE: Weigert, Martin, et al. \"Content-aware image restoration: pushing the limits of fluorescence microscopy.\" Nature methods 15.12 (2018): 1090-1097.'\n","  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n","\n","  pdf.ln(3)\n","  reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n","\n","  pdf.set_font('Arial', size = 11, style='B')\n","  pdf.multi_cell(190, 5, txt=reminder, align='C')\n","\n","  pdf.output(full_QC_model_path+'Quality Control/'+QC_model_name+'_QC_report.pdf')\n","\n","\n","# Exporting requirements.txt for local run\n","!pip freeze > requirements.txt\n","\n","after = [str(m) for m in sys.modules]\n","# Get minimum requirements file\n","\n","#Add the following lines before all imports: \n","#  import sys\n","#  before = [str(m) for m in sys.modules]\n","\n","#Add the following line after the imports:\n","#  after = [str(m) for m in sys.modules]\n","\n","from builtins import any as b_any\n","\n","def filter_files(file_list, filter_list):\n","    filtered_list = []\n","    for fname in file_list:\n","        if b_any(fname.split('==')[0] in s for s in filter_list):\n","            filtered_list.append(fname)\n","    return filtered_list\n","\n","df = pd.read_csv('requirements.txt', delimiter = \"\\n\")\n","mod_list = [m.split('.')[0] for m in after if not m in before]\n","req_list_temp = df.values.tolist()\n","req_list = [x[0] for x in req_list_temp]\n","\n","# Replace with package name \n","mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n","mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list] \n","filtered_list = filter_files(req_list, mod_replace_list)\n","\n","\n","file=open('CARE_2D_requirements_simple.txt','w')\n","for item in filtered_list:\n","    file.writelines(item + '\\n')\n","\n","file.close()\n","\n","import pandas as pd\n","from skimage import io,exposure, util\n","import matplotlib as plt\n","import numpy as np\n","!pip uninstall opencv-python-headless==4.5.5.62\n","!pip install opencv-python-headless==4.1.2.30\n","!pip install cellpose \n","from cellpose import models,io\n","# use_GPU = models.use_gpu()\n","from cellpose import plot\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QguRaoY_97c","cellView":"form"},"outputs":[],"source":["#@markdown ####*This is only needed if above block throws an error with opencv (cv2)\n","# if error is thrown, comment out all lines below numpy install and run as separate block here\n","\n","!pip uninstall opencv-python-headless==4.5.5.62\n","!pip install opencv-python-headless==4.1.2.30\n","!pip install cellpose \n","from cellpose import models,io\n","from cellpose import plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q9J4ihX6H-l"},"outputs":[],"source":["#@markdown ####Load all other packages and general functions\n","\n","# add various packages that aren't typically in colab\n","%%capture\n","!pip install mahotas\n","!pip install ortools==9.3.10459\n","!pip install nd2reader\n","\n","# load other packages.\n","import cv2\n","import dill\n","from functools import reduce\n","import mahotas\n","import matplotlib.pyplot as plt\n","import nd2reader\n","from numpy import genfromtxt\n","from scipy import ndimage as ndi\n","from scipy.spatial.distance import cdist\n","from skimage import color, feature, filters, io, measure, morphology, restoration, segmentation, exposure, restoration, data, exposure,transform\n","from skimage.feature import greycomatrix, greycoprops,hog\n","from skimage.segmentation import clear_border\n","from sklearn.metrics.pairwise import euclidean_distances\n","from sklearn.neighbors import NearestNeighbors\n","import sys\n","import warnings\n","\n","\n","# import additional function related to connecting cells.\n","sys.path.insert(0,\"/content/drive/MyDrive/FateTrack_Main/FateTrack_master/FateTrackScripts/\")\n","from FT_connect_functions import *\n","\n","# list of all features that are calculated using scikit image\n","featureColumns = ('label','area','filled_area','bbox','bbox_area', 'centroid','convex_area','eccentricity',\n","                      'solidity','convex_area','mean_intensity','min_intensity','filled_area',\n","                      'max_intensity','orientation','major_axis_length','minor_axis_length','euler_number',\n","                      'perimeter','extent','equivalent_diameter','feret_diameter_max','perimeter_crofton',\n","                      'moments_central','moments_hu','moments','moments_normalized',\n","                      'inertia_tensor','inertia_tensor_eigvals', 'intensity_image')\n","\n","####!!!!####\n","\n","# specify the segmentation mask and channel-based Image (will calculate fluoresence features as well).\n","def collectObjectTraits(segmentationsMask, channelImage, prefix=\"\", time=0, addExtras=False, addNeighbors=False, maxsize=100):\n","    \"\"\"Collect all features from a segmentation mask.\n","\n","    Parameters\n","    __________\n","    segmentationsMask: array\n","      masks from segmentation, to be used to calculate features\n","    channelImage: array\n","      raw image\n","    prefix: str, default = \"\"\n","      prefix for features names when added to feature vector\n","    time: float, default = 0\n","      DESCRIPTION_HERE\n","    addExtras: bool, default = False\n","      whether to add additional object traits\n","    maxsize: int, default = 100\n","      size for padded image, passed into extraObjectTraits() if addExtras = true\n","    \n","    Returns\n","    _______\n","    final: dataframe\n","      features for all cells in image\n","    \"\"\"\n","    image_props = pd.DataFrame(measure.regionprops_table(segmentationsMask, intensity_image=channelImage,properties=featureColumns)) # generate standard measurements with properties listed above.\n","    im_df = pd.DataFrame(image_props) # convert object to dataframe - is it not already a dataframe in the previous line?\n","\n","    # extra features are added below (if specified)\n","    if (addExtras or addNeighbors):\n","      if (addExtras):\n","        additionProps = pd.DataFrame() # set up dataframe\n","        for object in range(im_df.shape[0]):\n","            tmpProps = extraObjectTraits(im_df['intensity_image'][object],maxsize=maxsize) # performs extra measurements on each cell.\n","            additionProps = additionProps.append(tmpProps) # appends extra features from each cell into single object.\n","\n","        additionProps = additionProps.reset_index(drop=True) # reseting index allows for proper merging.\n","        im_df = pd.concat([im_df, additionProps], axis=1) # concatenate extra features into main feature frame.\n","      if (addNeighbors):\n","        im_df = getNeighborFeatures(im_df) # calculates neighbor features for each cell\n","\n","    im_df = im_df.add_prefix(prefix) # add prefixes if they exist.\n","    im_df = im_df.rename(columns={prefix+'label':'label'})  # relabel with prefixes if they exist.\n","    im_df['time'] = time # specify time component.\n","    return(im_df)\n","\n"," \n","def extraObjectTraits(intensityImage, maxsize=100):\n","    \"\"\"Function to load in additional object traits.\n","\n","    Parameters\n","    __________\n","    intensityImage: 2d array\n","      raw image\n","    maxsize: int, default = 100\n","      size for padded image\n","    \n","    Returns\n","    _______\n","    dataframe\n","      extra image features\n","    \"\"\"\n","    im_stretched = mahotas.stretch(intensityImage) # linear stretch of intensities from 0 to 255\n","\n","    zernike = mahotas.features.zernike(im_stretched, radius=20, degree=8, cm=mahotas.center_of_mass(im_stretched))\n","    zern_df = pd.DataFrame(zernike).T\n","    zern_df.columns = ['zern_'+ sub for sub in list(map(str,zern_df.columns))]\n","\n","    haralick = mahotas.features.haralick(im_stretched,return_mean=True)\n","    hara_df = pd.DataFrame(haralick).T\n","    hara_df.columns = ['hara_'+ sub for sub in list(map(str,hara_df.columns))]\n","    width,height = np.shape(im_stretched)\n","\n","    h1 = int(np.floor((maxsize-height)/2))\n","    h2 = int(np.ceil((maxsize-height)/2))\n","    w1 = int(np.floor((maxsize-width)/2))\n","    w2 = int(np.ceil((maxsize-width)/2))\n","    padIm =(np.pad(im_stretched,pad_width=((w1,w2),(h1,h2)),mode='constant'))\n","\n","    hog_fd = hog(padIm, orientations=20, pixels_per_cell=(25, 25), cells_per_block=(2, 2), visualize=False, multichannel=False,feature_vector=True) # histogram of oriented gradients\n","    hog_df = pd.DataFrame(hog_fd).T\n","    hog_df.columns = ['hog_'+ sub for sub in list(map(str,hog_df.columns))]\n","\n","    return(pd.concat([hog_df, hara_df,zern_df], axis=1))\n","\n","def getNeighborFeatures(im2_df):\n","  \"\"\" Function to calculate neighborhood features.\n","\n","  Parameters\n","  __________\n","  sub_tab: dataframe\n","    contains centroid coordinates for each nucleus\n","\n","  Returns\n","  _______\n","  im2_df: dataframe\n","    contains neighborhood features for each nucleus\n","  \"\"\"\n","  sub_tab = im2_df[[\"centroid-0\", \"centroid-1\"]] # df of centroids\n","  nn = NearestNeighbors(n_neighbors = sub_tab.shape[0], radius = 50) # create nearest neighbor model\n","  nn.fit(sub_tab) # find the nearest neighbors\n","  nn_dist, nn_ind = nn.radius_neighbors(sub_tab) # find distances to each other nucleus and the indices of the neighbors in 50px radius\n","\n","  # look at distances between every pair of nuclei and take the minimum\n","  # must calculate desparately from nn to account for having no neighbors in 50px radius\n","  mindist = []\n","  dist = euclidean_distances(sub_tab) # Euclidean distances between all centroids\n","  for i in range(dist.shape[0]):\n","    dist_i = dist[i]\n","    dist_i = dist_i[dist_i > 0] # filter out the nucleus itself\n","    mindist.append(min(dist_i)) # add the minimum distance to list\n","  im2_df[\"mindist\"] = mindist # append min distances to the feature frame\n","  \n","  # treat every nucleus as a point (centroid), count all other nuclei within given radius\n","  nearby = []\n","  for i in range(nn_ind.shape[0]):\n","      nearby.append(nn_ind[i].shape[0]) # find the number of nuclei within 50px radius\n","  im2_df[\"nearby.cells\"] = nearby\n","\n","  ## nearest neighbor features - can be anything from the full list of features (but might want to avoid the inertia features)\n","  colOfInterest = [\"area\", \"eccentricity\", \"solidity\", \"convex_area\"\n","                ,\"orientation\", \"major_axis_length\",\"minor_axis_length\", \"perimeter\", \"extent\"]\n","  for col in colOfInterest:\n","    colMin = []\n","    colMax = []\n","    for i in range(nn_ind.shape[0]):\n","      if nn_ind[i].shape[0] == 0:\n","        colMin.append(im2_df.loc[i, col])\n","        colMax.append(im2_df.loc[i, col])\n","      else:\n","        min_all = sys.maxsize\n","        max_all = 0\n","        for j in nn_ind[i]:\n","          feat = im2_df.loc[j, col]\n","          if feat < min_all:\n","            min_all = feat\n","          if feat > max_all:\n","            max_all = feat\n","        colMin.append(min_all)\n","        colMax.append(max_all)\n","    im2_df[\"nearby_\" + col + \"_min\"] = colMin\n","    im2_df[\"nearby_\" + col + \"_max\"] = colMax\n","\n","  return im2_df\n","\n","#Often images are too large to process with colab, it helps to break them into smaller trucks for processing.\n","def subDivideImage(imageFilename,div = 4):\n","  \"\"\"Split image into smaller blocks for processing. This function is not used in processing but is helpful for your own use.\n","\n","  Parameters\n","  __________\n","  imageFilename: str\n","    filename of larger image\n","  div: int\n","    number of divisions of the image along one dimension, total number of blocks will be div^2\n","\n","  Returns\n","  _______\n","  imageList: list of 2d arrays\n","    list of the smaller images\n","  \"\"\"\n","  image=io.imread(imageFilename, plugin=\"pil\") # read in image file\n","  newDim = int(np.floor(np.min([image.shape[1],image.shape[2]])/div)) # determine the image dimenion for a given subdivision dize.\n","  imageList = [] \n","  for i in range(div):   # break image into parts and save each subdivided image into a list.\n","    for ii in range(div):\n","      tmp = util.img_as_uint(image[:,(i*newDim):((i+1)*newDim),(ii*newDim):((ii+1)*newDim)])\n","      imageList.append(tmp)\n","  return(imageList)\n","\n","# older function used to process images as above^ but for ND2s.\n","def subDivideND2(image,div = 4):\n","  \"\"\"Split nd2 image into smaller blocks for processing. This function is not used in processing but is helpful for your own use.\n","\n","  Parameters\n","  __________\n","  imageFilename: nd2 object\n","    nd2 image, already loaded\n","  div: int\n","    number of divisions of the image along one dimension, total number of blocks will be div^2\n","\n","  Returns\n","  _______\n","  imageList: list of 2d arrays\n","    list of the smaller images\n","  \"\"\"\n","  newDim = int(np.floor(np.min([image.shape[1],image.shape[2]])/div))\n","  imageList = []\n","  for i in range(div):\n","    for ii in range(div):\n","      tmp = util.img_as_uint(image[:,(i*newDim):((i+1)*newDim),(ii*newDim):((ii+1)*newDim)])\n","      imageList.append(tmp)\n","  return(imageList)\n","# defualt dettings use GPU and a nuclear diameter set to 21 pixels.\n","def imAdjust(I,low_in,high_in,low_out,high_out,gamma=1):\n","    \n","    # Similar to imadjust in MATLAB.\n","    # Converts an image range from [low_in,high_in] to [low_out,high_out].\n","    # The Equation of a line can be used for this transformation:\n","    #   y=((high_out-low_out)/(high_in-low_in))*(I-low_in)+low_out\n","    # However, it is better to use a more generalized equation:\n","    #   y=((I-low_in)/(high_in-low_in))^gamma*(high_out-low_out)+low_out\n","    # If gamma is equal to 1, then the line equation is used.\n","    # When gamma is not equal to 1, then the transformation is not linear.\n","\n","    return (((I - low_in) / (high_in - low_in)) ** gamma) * (high_out - low_out) + low_out\n","\n","\n","def generateDisplacement(image1,image2):\n","    \"\"\"General function to generate the displacement vector between two images using phase cross correlation.\n","\n","    Parameters\n","    __________\n","    image1 : array\n","      reference image\n","    image2 : array\n","      moving image\n","    \n","    Returns\n","    _______\n","    ndarray\n","      displacement in y in pixels, displacement in x in pixels\n","    \"\"\"\n","    shift_vector = registration.phase_cross_correlation(image1,image2) # generates displacements, from skimage\n","    return(shift_vector[0])\n","\n","\n","\n","# default settings use GPU and a nuclear diameter set to 21 pixels.\n","def segmentNuclei(imageList, modelCellpose=None, useGPU = True, flowThreshold= 0.5, cellProbabilityThreshold= 1.0,nuclearDiameter= 21.0, channels= [0,0]):\n","  \"\"\"Wrapper function to segment images using cellpose.\n","\n","  Parameters\n","  __________\n","  imageList : list\n","    list of images to segment\n","  modelCellpose : string, default = None\n","    Cellpose model to use, specified above\n","  useGPU : bool, default = True\n","    whether to use GPU\n","  flowThreshold : float, default = 0.5\n","    flow error threshold for cellpose\n","  cellProbabilityThreshold : float, default = 1.0\n","    DESCRIPTION HERE\n","  nuclearDiameter : float, default = 21.0\n","    average nuclear diameter for cellpose\n","  channels : list, default = [0, 0]\n","    list of channels, either of length 2 or of length number of images by 2. \n","    First element of list is the channel to segment (0=grayscale, 1=red, 2=green, 3=blue). \n","    Second element of list is the optional nuclear channel (0=none, 1=red, 2=green, 3=blue).\n","    [0, 0] for grayscale images.\n","  \n","  Returns\n","  _______\n","  masks : list of 2d arrays\n","    masks\n","  \"\"\"\n","  if (modelCellpose != None): # use provided model for Cellpose\n","    masks, flows, styles = modelCellpose.eval(imageList, diameter=nuclearDiameter, flow_threshold=flowThreshold,\n","                                          cellprob_threshold=cellProbabilityThreshold, channels=channels)\n","  else:\n","    modelCellpose = models.Cellpose(model_type='nuclei', gpu=useGPU) # use nucleus model if none provided\n","    masks, flows, styles, diams = modelCellpose.eval(imageList, diameter=nuclearDiameter, flow_threshold=flowThreshold,\n","                                          cellprob_threshold=cellProbabilityThreshold, channels=channels)\n","\n","  return(masks)\n","\n","\n","def generateConnections(frame_0, frame_1, shiftVector,time=0, allowSplitting=True):\n","  \"\"\"Generate connections between cells in consecutive timelapse frames.\n","\n","  Parameters\n","  __________\n","  frame_0: dataframe\n","      A dataframe containing the information (cell IDs, positions, sizes, etc...) for all cells at current frame.\n","  frame_1: dataframe\n","      A dataframe containing the information (cell IDs, positions, sizes, etc...) for all cells at next frame.\n","  shiftVector: array\n","      An array contianing the overall displacement vector between two frames.\n","  time: int, default = 0\n","    timepoint of initial frame\n","  allowSplitting: bool, default = True\n","    whether to allow for dividing cells\n","\n","  Returns\n","  _______\n","  final: dataframe\n","    connections\n","  \n","  \"\"\"\n","  t01, candies, costMatrix1 = getCostMatrix(frame_0, frame_1,shiftVec =shiftVector) # potential future location for each cell, candidates for future cells, cost matrix for each of those candidates\n","  if (allowSplitting):\n","    mitosisCandidates =getMitosisCandidates(CandidateFrame=candies,FeatureFrame_t0=t01,FeatureFrame_t1=frame_1) # find candidates for future cells if splitting is allowed\n","    if (mitosisCandidates.shape[0]>1):\n","        mitosisConnections = np.unique(np.append(np.delete(mitosisCandidates, 1, 1),\n","                                                  np.delete(mitosisCandidates, 2, 1),axis=0),axis=0) # WHAT DOES THIS LINE DO\n","        for i in range(mitosisConnections.shape[0]):\n","            costMatrix1[mitosisConnections[i,1],mitosisConnections[i,0]] = (costMatrix1[mitosisConnections[i,1],mitosisConnections[i,0]]/2)\n","\n","        mitosisCandidateNodes = np.unique(mitosisConnections[:,0])\n","\n","        FinFrame = solveMinCostFlow(CostMatrix=costMatrix1, mitosisCands=mitosisCandidateNodes)\n","    else:\n","        FinFrame = solveMinCostFlow(CostMatrix=costMatrix1,mitosisCands=[])\n","  else:\n","    FinFrame = solveMinCostFlow(CostMatrix=costMatrix1,mitosisCands=[])\n","\n","  final = formatFinalConnections(Connections=FinFrame,FeatureFrame_t0=t01,FeatureFrame_t1=frame_1,timept_0=str(time), timept_1=str(time+1))\n","  return(final)\n","\n","def solveMinCostFlowFixed(CostMatrix,mitosisCands):\n","    \"\"\"Solves the global minimum cost by assigning cells in frame n to counterparts in frame n+1.\n","\n","    Parameters\n","    __________\n","    CostMatrix: array\n","      An array containg the cost of connecting a cell in frame n to an allowed cell in frame n+1.\n","    mitosisCands: list\n","      A list containing all cells that can possibly divide between two frames as determined by properties of nuclei.\n","\n","    Returns\n","    _______\n","    FinalFrame: dataframe\n","      A dataframe that gives the start cell(node) and end cell(node) after solving the min cost flow. \n","    \"\"\"\n","    global openingCost, closingCost\n","\n","    t0Nodes = np.array(range(CostMatrix.shape[1]))+1  # generate array of numbers designating all start nodes (from 1 to n, where n is number of start nodes)\n","    t1Nodes = np.array(range(CostMatrix.shape[1],np.sum(CostMatrix.shape)))+1 # generate array of numbers designating all end nodes (from n+1 to n+m+1, where n is number of start nodes and m is number of n nodes)\n","\n","    # need to generate all possible connections. Shape of network resembles.\n","    #      ---1-----4---\n","    #.    /     \\ /     \\\n","    #    0----2--X--5----7\n","    #     \\     / \\     /\n","    #.     ---3-----6---\n","    # Where 0 is the source, and 7 is the sink.\n","\n","    # We begin to build the structure for computing the min cost flow.\n","\n","    # Give all possible connections by creating all start nodes (including repeats)\n","    start_nodes = np.concatenate((np.repeat([0],np.sum(CostMatrix.shape)), # all connections to source node\n","                                  (np.nonzero(CostMatrix))[1]+1, # all connections from t0 to t1\n","                                  t0Nodes,t1Nodes # all connections to sink node\n","                                 )).tolist()\n","    \n","    # Give all possible connections by creating all start end (including repeats)\n","    end_nodes = np.concatenate((t0Nodes,t1Nodes, # all connections to source node\n","                                np.nonzero(CostMatrix)[0]+int(CostMatrix.shape[1])+1, # all connections from t0 to t1\n","                                np.repeat([np.sum(CostMatrix.shape)+1],np.sum(CostMatrix.shape)) # all connections to sink node\n","                                 )).tolist()\n","    \n","    # Generate a list of costs for all connections using the cost matrix. The cost of going from the source node and to sink node low (1).\n","    costs = np.concatenate((np.repeat(1,CostMatrix.shape[1]), # all connections to source node\n","                            np.repeat(openingCost,CostMatrix.shape[0]),\n","                            CostMatrix[CostMatrix!=0], # all connections from t0 to t1\n","                            np.repeat(closingCost,CostMatrix.shape[1]), # all connections to sink node\n","                            np.repeat(1,CostMatrix.shape[0]) # all connections to sink node\n","                            )).tolist()\n","\n","    # Generate the capacities for each node.\n","    nodeCaps = np.concatenate((t0Nodes,t1Nodes),axis=0)\n","    nodeCaps = np.vstack((nodeCaps, np.repeat(1,len(nodeCaps)))).T\n","\n","    # If the node is a  mitosis candidate (can generate two connections), then we replace the capacity with a value of 2.\n","    if(len(mitosisCands)>0):\n","        nodeCaps[np.searchsorted(nodeCaps[:,0],mitosisCands+1),1]=2\n","\n","    # Creates the capacities list.\n","    capacities = np.concatenate((nodeCaps[:,1], # all connections to source node\n","                                 np.repeat(1,np.sum(CostMatrix[CostMatrix!=0].shape)),# all connections from t0 to t1\n","                                np.repeat(1,np.sum(CostMatrix.shape)) # all connections to sink node\n","                                )).tolist()\n","\n","    # Generate the supplies at the source node and sink node. We do not allow for sources or sinks outside of the 0th and (n+m+1)th nodes.\n","    supply_amount = np.min([CostMatrix.shape[1],CostMatrix.shape[0]])#np.max([CostMatrix.shape[0],CostMatrix.shape[1]])\n","    supplies = np.concatenate(([supply_amount],np.repeat(0,np.sum(CostMatrix.shape)),[-1*supply_amount])).tolist()\n","\n","\n","    min_cost_flow = pywrapgraph.SimpleMinCostFlow() # creates the min cost flow object.\n","\n","    # Add each arc within the min cost flow object. \n","    # The arc subobject contain a start and end node, a capacity, and a cost.\n","    for i in range(len(start_nodes)):\n","        min_cost_flow.AddArcWithCapacityAndUnitCost(start_nodes[i], end_nodes[i],capacities[i], int(costs[i]))\n","\n","    # Add the supply to each node.\n","    for i in range(len(supplies)):\n","        min_cost_flow.SetNodeSupply(i, supplies[i])\n","\n","\n","    ArcFrame = pd.DataFrame() # creates a dataframe object for dinal export.\n","\n","    # Find the minimum cost flow between node 0 and node 4.\n","    if min_cost_flow.Solve() == min_cost_flow.OPTIMAL:\n","        print('Minimum cost:', min_cost_flow.OptimalCost())\n","        for i in range(min_cost_flow.NumArcs()):\n","            cost = min_cost_flow.Flow(i) * min_cost_flow.UnitCost(i) # cost is defined as the flow (number of units across a connection) * the cost of the connection.\n","\n","            ArcFrame = ArcFrame.append(pd.DataFrame([min_cost_flow.Tail(i),\n","              min_cost_flow.Head(i),\n","              min_cost_flow.Flow(i),\n","              min_cost_flow.Capacity(i),\n","              cost]).T)\n","    else:\n","        print('There was an issue with the min cost flow input.')\n","\n","    ArcFrame = ArcFrame.rename(columns={0:'start',1:'end',2:\"Flow\",3:\"Capacity\",4:\"Cost\"}) #rename columns\n","    FinalFrame = ArcFrame.loc[ArcFrame[\"Flow\"]!=0,] #remove all arcs that are unused.\n","    FinalFrame = FinalFrame[(FinalFrame[\"start\"]!=0)&(FinalFrame[\"end\"]!=(np.sum(CostMatrix.shape)+1))] #remove all arcs that start with the source and the sink.\n","    FinalFrame['end']=(FinalFrame['end']-int(CostMatrix.shape[1])-1)\n","    FinalFrame['start']=(FinalFrame['start']-1) # rename the nodes \n","    FinalFrame = FinalFrame.reset_index(drop=True) # reset the index so the frames last index is the number of rows.\n","    return(FinalFrame)\n","\n","def getCostMatrix(FeatureFrame_t0, FeatureFrame_t1, shiftVec):\n","    \"\"\"Generate the cost matrix for all possible cells in current frame to cells in next frames.\n","\n","    Parameters\n","    __________\n","    FeatureFrame_t0: dataframe\n","      A dataframe containing the information (cell IDs, positions, sizes, etc...) for all cells at current frame.\n","    FeatureFrame_t1: dataframe\n","      A dataframe containing the information (cell IDs, positions, sizes, etc...) for all cells at next frame.\n","    shiftVec: array\n","      An array contianing the overall displacement vector between two frames.\n","\n","    Returns\n","    _______\n","    FeatureFrame_t0: dataframe\n","      A dataframe with updated information for the current time.\n","    CandidateMtx: array\n","      A matrix that associated any current cell with any possible connection at the next time point.\n","    costMatrix: array\n","      A matrix listing all the costs in connecting a current cell with a future cell.\n","    \"\"\"\n","    global track_frameExpCoeff, costIntCoefficient, costSizeCoefficient, costPositionCoefficient\n","\n","\n","#### -> LAILA \n","    FeatureFrame_t0 = expandBoundBox(FeatureFrame_t0, expansion = track_frameExpCoeff) # calculate the bound box to determine potential future cells.\n","    FeatureFrame_t0 = futureBoundBox(FeatureFrame_t0, shiftVector=shiftVec) # calculate the bound box when you take into account the shift between the two frames.\n","    CandidateMtx = genCandidateNodes(FeatureFrame_t0,FeatureFrame_t1) # generate all possible future cells for each current cell.\n","    FeatureFrame_t0 = expectedLocation(FeatureFrame_t0, shiftVector=shiftVec) # generate potential future locations for each cell.\n","    deltaPosition = getDifference(FeatureFrame_t0,FeatureFrame_t1,\"position\") # find the difference in position between each current cell and all possible future cells. This weighs into cost.\n","    deltaArea = getDifference(FeatureFrame_t0,FeatureFrame_t1,\"area\") # find the difference in area between each current cell and all possible future cells. This weighs into cost.\n","\n","    costMatrix = ((costSizeCoefficient*(deltaArea)+costPositionCoefficient*(deltaPosition))*CandidateMtx) #Use a linear combination of differences in current and future cellular properties to calculate cost of each connection.\n","    return((FeatureFrame_t0,CandidateMtx,costMatrix))\n","\n","\n","def generateConnectionsFixed(frame_0, frame_1, shiftVector,time=0, allowSplitting=True):\n","  t01, candies, costMatrix1 = getCostMatrix(frame_0, frame_1,shiftVec =shiftVector)\n","  if (allowSplitting):\n","    mitosisCandidates =getMitosisCandidates(CandidateFrame=candies,FeatureFrame_t0=t01,FeatureFrame_t1=frame_1)\n","    if (mitosisCandidates.shape[0]>1):\n","        mitosisConnections = np.unique(np.append(np.delete(mitosisCandidates, 1, 1),\n","                                                  np.delete(mitosisCandidates, 2, 1),axis=0),axis=0)\n","\n","        for i in range(mitosisConnections.shape[0]):\n","            costMatrix1[mitosisConnections[i,1],mitosisConnections[i,0]] = (costMatrix1[mitosisConnections[i,1],mitosisConnections[i,0]]/2)\n","\n","        mitosisCandidateNodes = np.unique(mitosisConnections[:,0])\n","\n","        FinFrame = solveMinCostFlow(CostMatrix=costMatrix1, mitosisCands=mitosisCandidateNodes)\n","    else:\n","        FinFrame = solveMinCostFlow(CostMatrix=costMatrix1,mitosisCands=[])\n","  else:\n","    FinFrame = solveMinCostFlowFixed(CostMatrix=costMatrix1,mitosisCands=[])\n","    \n","  final = formatFinalConnections(Connections=FinFrame,FeatureFrame_t0=t01,FeatureFrame_t1=frame_1,timept_0=str(time), timept_1=str(time+1))\n","  return(final)\n","\n","def save_object(obj, filename):\n","    \"\"\"Save a FateTrack object to a .pkl file.\n","\n","    Parameters\n","    __________\n","    obj: FateTrack object.\n","      The filename for the .pkl object.\n","    filename: str\n","      The filename for the .pkl object.\n","    \"\"\"\n","    with open(filename, 'wb') as output:  # Overwrites any existing file.\n","        dill.dump(obj, output)\n","\n","def load_object(filename):\n","    \"\"\"Load a pickle object.\n","\n","    Parameters\n","    __________\n","    filename: str\n","      The filename for the .pkl object.\n","\n","    Returns\n","    _______\n","    tmp: FateTrack\n","      The FateTrack object.\n","    \"\"\"\n","    with open(filename, 'rb') as red:  # Overwrites any existing file.\n","        tmp = dill.load(red)\n","    return(tmp)\n","\n","def templateCall(smallerImage, largerImage):\n","  \"\"\"Match cells between timelapse and HCR rounds.\n","\n","  Parameters\n","  __________\n","  smallerImage: array\n","    smaller of the images to match\n","  largerImage: array\n","    larger of the images to match\n","\n","  Returns\n","  _______\n","  newDims0: tuple of int\n","    new y min coordinate, new y max coordinate\n","  newDims1: tuple of int\n","    new x min coordinate, new x max coordinate\n","  matchGrid.argmax(): cross correlation score where template is matched\n","  \"\"\"\n","  matchGrid = cv2.matchTemplate(smallerImage.astype('float32'),largerImage.astype('float32'),cv2.TM_CCOEFF_NORMED) # find smaller (template) image in larer image\n","  newImSpot = np.unravel_index(matchGrid.argmax(),matchGrid.shape) # find where the template matches best\n","  newDims0 = newImSpot[0], newImSpot[0]+smallerImage.shape[0] # y coordinates\n","  newDims1 = newImSpot[1], newImSpot[1]+smallerImage.shape[1] # x coordinates\n","  return([newDims0,newDims1],np.amax(matchGrid))\n","\n","\n","def getDilatedMasks(maskImage, round=\"Rd1\",dilationPixels=3):\n","  \"\"\" Finds dilated masks.\n","\n","  Parameters\n","  __________\n","  maskImage: ndarray\n","    image of masks\n","  round: str, default = \"Rd1\"\n","    which HCR rounds the masks are being dilate dfor, used as a prefix in final dataframe\n","  dilationPixels: float, default = 3\n","    number of pixels to dilate masks by\n","\n","  Returns\n","  _______\n","  dilationMasks: ndarray\n","    dilated masks\n","  points_frame: dataframe\n","    contains the pre-dilated and post-dilated labels for the masks\n","  \"\"\"\n","  points_frame = pd.DataFrame(measure.regionprops_table(maskImage, properties=['label','centroid'])) # get label and centroid based on masks\n","  pointList = np.array(points_frame.iloc[:,-2:].astype(int)) # save the centroids, rounded off to integers, in n x 2 array\n","  masks=morphology.dilation((maskImage>0),morphology.disk(dilationPixels)) # dilate the masks\n","  secmask = np.zeros(masks.shape, dtype=bool) # make array of False in shape of image\n","  secmask[tuple(pointList.T)] = True # change to True where the centroids are located\n","  secmaskDil = morphology.dilation(secmask,morphology.square(3)) # create square of size 3 around the each centroid\n","  markers, _ = ndi.label(secmaskDil) # numbers each nucleus\n","  dilationMasks = segmentation.watershed( -secmask.astype(int), markers, mask=(masks>0)) # perform watershed on the nuclei\n","\n","  points_frame['dilated_label'] = [dilationMasks[pointList[x,0],pointList[x,1]] for x in range(pointList.shape[0])] # finds the dilated label corresponding to each centroid\n","  points_frame = points_frame.rename(columns={'label' : 'orig_label'}) # renames the label column to indicate that it is the orignal (pre-dilated) label\n","  points_frame = points_frame.drop(columns=['centroid-0', 'centroid-1'])\n","  points_frame = points_frame.add_prefix(str(round)+\"_\") # adds round prefix to the labels\n","  return(dilationMasks,points_frame)\n","\n","def quantiles(regionmask, intensity):\n","    return np.percentile(intensity[regionmask], q=(10, 25, 50, 75, 90))\n","\n","def measureIntProps(maskMtx,channel,channelImage):\n","    \"\"\" Measure HCR fluorescence intensities in all channels.\n","\n","    Parameters\n","    __________\n","    maskMtx: ndarray\n","      dilated masks\n","    channel: str\n","      channel in which intensity is being calculated\n","    channelImage: ndarray\n","      image for that channel\n","\n","    Returns\n","    _______\n","    props: dataframe\n","      contains intensity properties for the channel\n","    \"\"\"\n","    listprops = ('label','centroid','filled_area','min_intensity','mean_intensity','max_intensity') # intensity properties to calculate\n","    props = measure.regionprops_table(maskMtx, intensity_image=channelImage, properties=listprops,extra_properties = (quantiles,)) # calculates the properties and the intensity quantiles\n","    props = pd.DataFrame(props) # convert to dataframe\n","    props['sum_intensity'] =  props['filled_area'] * props['mean_intensity'] # integrate intensity over the image (mean)\n","    props['mean_intensity'] = props['mean_intensity']\n","    props = props.add_prefix(channel+'_') # add the channel name as a prefix to each of the properties in the dataframe columns\n","    props = props.rename(columns={channel+\"_label\": \"label\",\n","                                  channel+\"_centroid-0\": \"centroid-0\",\n","                                  channel+\"_centroid-1\": \"centroid-1\"}) # rename the label columns to have the channel\n","    return(props)\n","    \n","\n","def getCellTypes(meas_df, muc2_thresh = 0.002, chga_thresh = 0.002, clu_thresh = 0.002, aldob_thresh = 0.002, lyz1_thresh = 0.002) :\n","  \"\"\" Determines cell types using laplacian to threshold.\n","\n","  Parameters\n","  __________\n","  meas_df: dataframe\n","    contains nuclear intensity and laplacian measurements in all channels\n","  muc2_thresh: float, default = 0.002\n","    threshold density for histogram bins, cells with laplacian values in bins below this density are considered goblet cells\n","  chga_thresh: float, default = 0.002\n","    threshold density for histogram bins, cells with laplacian values in bins below this density are considered enteroendocrine cells\n","  clu_thresh: float, default = 0.002\n","    threshold density for histogram bins, cells with laplacian values in bins below this density are considered Clu-positive cells\n","  aldob_thresh: float, default = 0.002\n","    threshold density for histogram bins, cells with laplacian values in bins below this density are considered enterocytes\n","  lyz1_thresh: float, default = 0.002\n","    threshold density for histogram bins, cells with laplacian values in bins below this density are considered Paneth cells\n","\n","  Returns\n","  _______\n","  meas_df: dataframe\n","    measurement dataframe with column appended with celltype labels\n","  \"\"\"\n","  meas_df['celltype'] = \"none\" # add a column for celltype annotations\n","  thresh_density = {'1_YFP' : muc2_thresh, '1_CY5' : chga_thresh, '2_CY3' : clu_thresh, '2_YFP' : aldob_thresh, '2_CY5' : lyz1_thresh}\n","  # for each channel name\n","  for channel in thresh_density :\n","    col_string = 'lp_Rd' + channel + '_mean_intensity'\n","    mean_lp = meas_df[col_string] # get the column with the mean laplacian values for that channel\n","    hist, bin_edges = np.histogram(mean_lp, bins = np.arange(0, np.max(mean_lp) + 0.002, 0.002)) # create a histogram of laplacian values\n","    lp_thresh = hist / len(mean_lp) < thresh_density.get(channel) # determine whether the density is below the threshold\n","    # find the first bin that dips below the density threshold\n","    thresh_bin = 500\n","    for i in range(len(lp_thresh)) :\n","      if lp_thresh[i] :\n","        thresh_bin = i\n","        break\n","    if thresh_bin < 500 : # if there are bins below the threshold\n","      cutoff = bin_edges[thresh_bin] # determine cutoff laplacian value\n","      # append the label for this channel to the label column if its laplacian is above the set value\n","      for i in range(len(mean_lp)) :\n","        if mean_lp[i] > cutoff :\n","          if meas_df['celltype'][i] == \"none\" :\n","            meas_df['celltype'][i] = [channel]\n","          else :\n","            meas_df['celltype'][i].append(channel)\n","  return meas_df\n","\n","\n","def connectWithLocalXCor(displaced, num_frames, TL_last_mCherrymask_props, HCR_Rd1_mCherry_image, TL_last_mCherry_image, TL_last_mCherry_mask, HCR_Rd1_mCherry_mask, exp_pix_TL = 20, exp_pix_HCR = 50, xcor_threshold = 0.9):\n","  \"\"\" Connects nuclei in the last timelapse image to those in the first round of HCR (mCherry) by matching the images using cross-correlation in a localized area.\n","\n","  Parameters\n","  __________\n","  displaced : tuple\n","    translation between timelapse and HCR image based on global cross-correlation, (x,y) gives coordinates in HCR image that correspond to (0,0) in timelapse image\n","  num_frames : int\n","    number of timelapse frames\n","  TL_last_mCherrymask_props : DataFrame\n","    dataframe of properties of each nucleus from the last timelapse image (from regionprops)\n","  HCR_Rd1_mCherry_image : 2D ndarray\n","    HCR round 1 mCherry image\n","  TL_last_mCherry_image : 2D ndarray\n","    last timelapse mCherry image\n","  TL_last_mCherry_mask : 2D ndarray\n","    masks from the last timelapse mCherry image\n","  HCR_Rd1_mCherry_mask : 2D ndarray\n","    masks from the HCR round 1 mCherry image\n","  exp_pix_TL : int, default=20\n","    number of pixels to expand around the centroid in the time-lapse image, used as template for cross-correlation\n","  exp_pix_HCR : int, default=40\n","    number of pixels to expand around the centroid in the HCR image, match for time-lapse is searched within this image\n","  xcor_threshold : float, default=0.9\n","    threshold cross-correlation score to determine a match between nuclei\n","\n","  Returns\n","  _______\n","  TLtoHCR_connections : DataFrame\n","    contains the nuclear label in the time-lapse image and its corresponding label in the HCR image\n","  \"\"\"\n","  TLtoHCR_connections = pd.DataFrame()\n","  for i in range(len(TL_last_mCherrymask_props)):\n","    nuc_of_interest = TL_last_mCherrymask_props.loc[i] # get the timelapse nucleus properties\n","    tl_bbox = np.asarray([nuc_of_interest['bbox-0'], nuc_of_interest['bbox-1'], nuc_of_interest['bbox-2'], nuc_of_interest['bbox-3']]).astype(int) # get the bounding box in the TL image\n","    # expand the bounding box for use in the TL image\n","    min_row = int(tl_bbox[0] - exp_pix_TL)\n","    min_col = int(tl_bbox[1] - exp_pix_TL)\n","    max_row = int(tl_bbox[2] + exp_pix_TL)\n","    max_col = int(tl_bbox[3] + exp_pix_TL)\n","    # correct for edges\n","    if min_row < 0:\n","      min_row = 0\n","    if min_col < 0:\n","      min_col = 0\n","    if max_row > TL_last_mCherry_image.shape[0]:\n","      max_row = TL_last_mCherry_image.shape[0]\n","    if max_col > TL_last_mCherry_image.shape[1]:\n","      max_col = TL_last_mCherry_image.shape[1]\n","    bbox_exp_TL = [min_row, min_col, max_row, max_col]\n","\n","    # expand the bounding box for use in the HCR image, taking into account global displacement (measured with cross-correlation)\n","    min_row = int(tl_bbox[0] - exp_pix_HCR + displaced[0])\n","    min_col = int(tl_bbox[1] - exp_pix_HCR + displaced[1])\n","    max_row = int(tl_bbox[2] + exp_pix_HCR + displaced[0])\n","    max_col = int(tl_bbox[3] + exp_pix_HCR + displaced[1])\n","    # correct for edges\n","    if min_row > HCR_Rd1_mCherry_image.shape[0] or min_col > HCR_Rd1_mCherry_image.shape[1] or max_row < 0 or max_col < 0:  # displacement is out of timelapse frame for this nucleus\n","      continue\n","    if min_row < 0:\n","      min_row = 0\n","    if min_col < 0:\n","      min_col = 0\n","    if max_row > HCR_Rd1_mCherry_image.shape[0]:\n","      max_row = HCR_Rd1_mCherry_image.shape[0]\n","    if max_col > HCR_Rd1_mCherry_image.shape[1]:\n","      max_col = HCR_Rd1_mCherry_image.shape[1]\n","    bbox_exp_HCR = [min_row, min_col, max_row, max_col]\n","\n","    tl_subset = TL_last_mCherry_image[bbox_exp_TL[0]:bbox_exp_TL[2], bbox_exp_TL[1]:bbox_exp_TL[3]] # bbox around tl nucleus\n","    tl_subset_mask = TL_last_mCherry_mask[bbox_exp_TL[0]:bbox_exp_TL[2], bbox_exp_TL[1]:bbox_exp_TL[3]] # bbox in mask file\n","    hcr_subset = HCR_Rd1_mCherry_image[bbox_exp_HCR[0]:bbox_exp_HCR[2], bbox_exp_HCR[1]:bbox_exp_HCR[3]] # expanded bbox around tl centroid location in hcr image\n","    hcr_subset_mask = HCR_Rd1_mCherry_mask[bbox_exp_HCR[0]:bbox_exp_HCR[2], bbox_exp_HCR[1]:bbox_exp_HCR[3]] # expanded bbox in hcr mask file\n","    if tl_subset.shape[0] > hcr_subset.shape[0] or tl_subset.shape[1] > hcr_subset.shape[1]: # at edge of image--not enough overlap with global cross-correlation shift\n","      continue\n","    shift_dims, xcor = templateCall(tl_subset, hcr_subset) # template match the TL to HCR\n","    if xcor > xcor_threshold: # if high enough matching score (cross-correlation)\n","      hcr_new_bbox = hcr_subset_mask[shift_dims[0][0]:shift_dims[0][1], shift_dims[1][0]:shift_dims[1][1]] # shift and crop the hcr mask accordingly\n","      # find the HCR nucleus with the most overlap with the TL nucleus\n","      hcr_tl_overlap = hcr_new_bbox[tl_subset_mask == nuc_of_interest['label']] # find where there is overlap between the tl_mask and the hcr_mask in the bbox\n","      nucs, counts = np.unique(hcr_tl_overlap, return_counts = True) # count up the different values that overlap\n","      if np.size(nucs) == 0:  # strange occurrence where bbox has a size of 1\n","        print(nuc_of_interest['label'])\n","        continue\n","      ix_0 = np.argmin(nucs)  # find 0 among the values where there were overlaps\n","      if nucs[ix_0] == 0:\n","        nucs = np.delete(nucs, ix_0) # delete 0\n","        counts = np.delete(counts, ix_0)\n","      if nucs.size == 0:  # if 0 was the only thing, continue\n","        continue\n","      ix_max = np.argmax(counts)  # find the nucleus label with the most overlap (number of pixels)\n","      hcr_id = nucs[ix_max]\n","      # add to the connection matrix\n","      TLtoHCR_connections = pd.concat([TLtoHCR_connections, pd.DataFrame([str(num_frames - 1) + \"_\" + str(int(nuc_of_interest['label'])), hcr_id]).T])\n","  return TLtoHCR_connections\n","\n","\n","def mCherryToDAPITranslation(mCherry_props, DAPI_props):\n","  \"\"\" Find the corresponding nuclear labels in the HCR Round 1 mCherry and DAPI images.\n","\n","  Parameters\n","  _________\n","  mCherry_props: DataFrame\n","    contains properties of the nuclei in the mCherry image (label and centroid)\n","  DAPI_props: DataFrame\n","    contains properties of the nuclei in the DAPI image (label and centroid), comes directly from self.HCR_measurements\n","\n","  Returns\n","  _______\n","  label_translation: DataFrame\n","    contains the nuclear label in the mCherry image and its corresponding label in the DAPI image\n","  \"\"\"\n","  mCherry = mCherry_props\n","  DAPI = DAPI_props\n","  mCherry_nucs = np.asarray(mCherry[['centroid-0', 'centroid-1']])  # get centroid positions for mCherry nuclei\n","  DAPI_nucs = np.asarray(DAPI[['nuc_Rd1_centroid-0', 'nuc_Rd1_centroid-1']])  # get centroid positions for DAPI nuclei\n","  centroid_dist = cdist(mCherry_nucs, DAPI_nucs)  # find the Euclidean distance between each mCherry nucleus and each DAPI nucleus\n","  closest_DAPI_nucleus = np.argmin(centroid_dist, axis = 1)  # index of each of the closest DAPI nuclei\n","  closest_DAPI_label = DAPI.iloc[closest_DAPI_nucleus][['Rd1_orig_label_DAPI']]  # gets the labels of each closest DAPI nucleus in the right order to merge back with the mCherry nuclei\n","  label_translation = np.concatenate([np.asarray(mCherry[['label']]), np.asarray(closest_DAPI_label)], axis = 1)  # concatenate mCherry labels with corresponding DAPI labels\n","  label_translation = np.delete(label_translation, np.nonzero(np.amin(centroid_dist, axis = 1) > 10), axis = 0)  # remove rows where distance between the nuclei > 10 px\n","  label_translation = pd.DataFrame(label_translation, columns = ['Rd1_orig_label', 'Rd1_orig_label_DAPI'])  # convert to dataframe\n","  label_translation.drop_duplicates(subset = 'Rd1_orig_label_DAPI', keep = False, inplace = True)\n","  return label_translation\n","\n","\n","def createMasterDataframe(finalIDs, num_frames, timelapse_connections, timelapse_rlg, HCR_measurements, HCR_Rd1_mCherrymask_props):\n","  \"\"\" Create a DataFrame with the corresponding nuclear labels and centroids across all images (timelapse and HCR)\n","\n","  Parameters\n","  _________\n","  finalIDs: DataFrame\n","    corresponding nuclear labels across all images \n","  num_frames: int\n","    number of timelapse frames\n","  timelapse_connections: list of DataFrames\n","    each DataFrame in the list has the connections between conesecutive timelapse frames\n","  timelapse_rlg: DataFrame\n","    raw connections information (original labels, frame number, centroids, parents, connection annotation)\n","  HCR_measurements: DataFrame\n","    measurements for each DAPI nucleus in HCR (need label and centroid information)\n","  HCR_Rd1_mCherrymask_props: DataFrame\n","    nuclear properties for each mCherry nucleus in HCR (need label and centroid information)\n","\n","  Returns\n","  _______\n","  all_ids: DataFrame\n","    contains the corresponding labels and centroids for each nucleus across all images\n","  \"\"\"\n","  all_ids = pd.DataFrame(); # create master dataframe\n","  all_ids = pd.concat([all_ids, finalIDs]) # start by putting in existing finalIDs dataframe\n","  # add rlgIDs to frame\n","  for i in range(num_frames):\n","    rlg_name = 'rlgID_' + str(i)\n","    masterID_name = 'MasterID_' + str(i)\n","    if (i != num_frames - 1):\n","      connect_frame = timelapse_connections[i][[rlg_name, masterID_name]]\n","    else:\n","      connect_frame = timelapse_connections[i-1][[rlg_name, masterID_name]]\n","    all_ids = pd.merge(all_ids, connect_frame, on = masterID_name)\n","\n","  # add centroids to frame (timelapse)\n","  rlg_frame = timelapse_rlg[['pointID', 'xCoord', 'yCoord']]\n","  for i in range(num_frames):\n","    rlg_name = 'rlgID_' + str(i)\n","    x = 'xCoord_' + str(i)\n","    y = 'yCoord_' + str(i)\n","    rlg_frame.set_axis([rlg_name, x, y], axis = 1, inplace = True)\n","    all_ids = pd.merge(all_ids, rlg_frame, on = rlg_name)\n","\n","  # add centroids to frame (HCR mCherry Rd1)\n","  mCherry_1_frame = HCR_Rd1_mCherrymask_props[['label', 'centroid-0', 'centroid-1']]\n","  mCherry_1_frame.set_axis(['Rd1_orig_label', 'yCoord_Rd1_mCherry', 'xCoord_Rd1_mCherry'], axis = 1, inplace = True)\n","  all_ids = pd.merge(all_ids, mCherry_1_frame, on = 'Rd1_orig_label')\n","\n","  # add centroids to frame (HCR DAPI)\n","  hcr_frame = HCR_measurements[['Rd1_orig_label_DAPI', 'lp_Rd1_centroid-0', 'lp_Rd1_centroid-1', 'lp_Rd2_centroid-0', 'lp_Rd2_centroid-1']]\n","  hcr_frame.set_axis(['Rd1_orig_label_DAPI', 'yCoord_Rd1_DAPI', 'xCoord_Rd1_DAPI', 'yCoord_Rd2_DAPI', 'xCoord_Rd2_DAPI'], axis = 1, inplace = True)\n","  all_ids = pd.merge(all_ids, hcr_frame, on = 'Rd1_orig_label_DAPI')\n","\n","  return all_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"HzoEkTgiK5nd"},"outputs":[],"source":["#@markdown ####Load FateTrack class and FateTrack specific functions \n","\n","from datetime import datetime\n","import nd2reader\n","class FateTrack:\n","  \"\"\"Contains raw and transformed images, features, and intensity measurements for one block of one image (sample).\n","\n","  Attributes\n","  __________\n","  block : int\n","    the assigned number of block when the field of view is divided\n","  divDimension : int\n","    number of divisions to perform on the image in each dimension (e.g. 3 means each row and column would be split by 3, for 9 blocks)\n","  finalIDs : dataframe\n","    contains the IDs for each nucleus at each timepoint and HCR rounds, with dilation\n","  HCR1file : str\n","    the filename and path to the 1st round HCR nd2 file\n","  HCR2file : str\n","    the filename and path to the 2nd round HCR nd2 file\n","  HCR_channelList: array of str\n","    channel names in HCR nd2 files\n","  HCR_measurements : dataframe\n","    image-normalized mean and percentile measurements for round 1 and round 2 HCR images, along with cell type annotation\n","  HCR_Rd1_DAPI_mask: array of uint8\n","    matrix of segmentation mask from HCR Rd1 DAPI channel\n","  HCR_Rd1_images : list of array of float\n","    HCR Rd1 images after alignment\n","  HCR_Rd1_mCherry_image : array of uint16\n","    HCR Rd1 mCherry image after CARE\n","  HCR_Rd1_mCherry_mask : array of uint16\n","    matrix of segmentation mask from HCR Rd1 mCherry channel (A594)\n","  HCR_Rd2_DAPI_mask : array of uint8\n","    matrix of segmentation mask from HCR Rd2 DAPI channel\n","  HCR_Rd2_images : list of array of float\n","    HCR Rd2 images after alignment\n","  HCR1toHCR2_coords : tuple of float\n","    coordinates in large HCR2 image that correspond to top left corner of HCR1 image\n","  HCR1toHCR2_crosscor : float\n","    cross-correlation score for matching HCR1 to HCR2 image\n","  keepTrailingFrames : str\n","    number of frames to keep from the trailing end, for example, 9 would mean to keep the final 9 frames of the timelapse\n","  masterDF: dataframe\n","    contains RLG IDs, Master IDs, and centroids at all timepoints and HCR (raw and dilated)\n","  nuclearChannel : str\n","    name of the channel to for segmentation of the nuclear timelapse, e.g. ‘A594’\n","  num_frames : int\n","    number of frames in the timelapse series\n","  outPath : str\n","    path to folder for output of component files and .pkl file\n","  sample : str\n","    name of the sample and the output folder\n","  timelapse_connections : list of dataframes\n","    each dataframe shows the connection between nuclear masks between imaging rounds\n","  timelapse_displacements : list of arrays\n","    each array has 2-elements showing the translational shift between frames in the timelapse\n","  timelapse_nucFluor_features : list of dataframes\n","    each dataframe gives the features of fluorescence nucleus at a given time point\n","  timelapse_nuclear_images: list of arrays of float\n","    each item in list is a restored fluorescent image in the timelapse series\n","  timelapse_raw_nuclear_images : list of arrays of float\n","    each item in list is a raw (unrestored) fluorescent image in the timelapse series\n","  timelapse_nuclear_masks : list of arrays of int\n","    each item in list is a segmentation mask from cellpose corresponding to timelapse fluorescence image\n","  timelapse_nucPhase_features : list of dataframes\n","    each dataframe gives the features of brightfield nucleus at a given time point\n","  timelapse_phase_images : list of arrays of float\n","    each item in list is a brightfield/phase image in the timelapse series\n","  timelapse_rlg : dataframe\n","    Raj Lab Timelapse GUI style dataframe showing timelapse connections\n","  TLfile : str\n","    path to nd2 file for the timelapse\n","  TLtoHCR_connections : dataframe\n","    connects identity of nuclear masks in final timelapse frame to first round of HCR mCherry\n","  TLtoHCR1_coords : tuple of float\n","    coordinates in large HCR1 image that correspond to top left corner of timelapse image\n","  TLtoHCR1_crosscor : float\n","    cross-correlation score for matching timelapse to HCR1 image\n","  trim : int\n","    number of pixels removed from top of field of view\n","  \"\"\"\n","  def __init__(self, sample, block, divDimension, timelapseFile, HCRFile1,HCRFile2, outPath,nuclearChannel='A594', keepTrailingFrames = 'all', timelapseTopTrim = 0):\n","    \"\"\"Constructor for FateTrack object.\n","\n","    Parameters\n","    __________\n","    sample: str\n","      sample name\n","    block: int\n","      block from larger image\n","    divDimension: int\n","      total number of blocks\n","    timelapseFile: str\n","      path to timelapse images file\n","    HCRFile1: str\n","      path to HCR round 1 file\n","    HCRFile2: str\n","      path to HCR round 2 file\n","    outPath: str\n","      path to output processed images and final pkl file\n","    nuclearChannel: str, default = 'A594'\n","      channel name used for nuclear imaging\n","    keepTrailingFrames: str, default = 'all'\n","      number of trailing frames to keep\n","    timelapseTopTrim: int\n","      deprecated\n","    \"\"\"\n","    self.sample = sample\n","    self.trim = timelapseTopTrim\n","    self.block = block\n","    self.TLfile = timelapseFile\n","    self.HCR1file = HCRFile1\n","    self.HCR2file = HCRFile2\n","    self.divDimension = divDimension\n","    self.nuclearChannel = nuclearChannel\n","    self.timelapse_phase_images = []\n","    self.timelapse_nuclear_images = []\n","    self.timelapse_raw_nuclear_images = []\n","    self.timelapse_nuclear_masks = []\n","    self.HCR_Rd1_images = []\n","    self.HCR_Rd2_images = []\n","    self.HCR_channelList = []\n","    self.TLtoHCR1_coords = []\n","    self.HCR1toHCR2_coords = []\n","    self.keepTrailingFrames = keepTrailingFrames\n","    self.outPath = outPath+'/'+self.sample\n","    if(block >= divDimension**2):\n","      print(\"Warning block number exceeds number of divisions!\\n Setting block number to 0.\")\n","      self.block = 0\n","    if not os.path.exists(self.outPath):\n","      os.makedirs(self.outPath)\n","\n","\n","  def loadTimeLapse(self,modelCARE, forceReload=False):\n","    \"\"\"Parse timelapse images and perform restoration.\n","    \n","    Parameters\n","    __________\n","    modelCARE : str\n","      CARE model to use based on path specifications\n","    forceReload: bool, default = False\n","      whether to rerun the function\n","    \"\"\"\n","    if (not os.path.exists(self.outPath+\"/timelapse\") or forceReload): # if not run before or forced to rerun\n","      tmp_timelapse = nd2reader.reader.ND2Reader(self.TLfile) # load in timelapse ND2 file\n","      num_frames = tmp_timelapse.metadata['total_images_per_channel']\n","      if (self.keepTrailingFrames == 'all'): # keeping all frames\n","        self.num_frames = num_frames\n","        frameRange = range(num_frames)\n","      else: # keeping only the last few frames, specified by self.keepTrailingFrames\n","        self.num_frames = int(self.keepTrailingFrames)\n","        frameRange = range(num_frames-int(self.keepTrailingFrames),num_frames) # creates range with the number corresponding to the frames to include\n","      \n","      channelList = np.asarray(tmp_timelapse.metadata['channels'])\n","      nuclearIndex = np.where(channelList==self.nuclearChannel)[0][0]\n","      BFIndex = np.where(channelList=='Brightfield')[0][0]\n","      dim1,dim2 = tmp_timelapse.metadata['width'],tmp_timelapse.metadata['height'] # current dimensions of image\n","      newDim = int(np.floor(np.min([dim1,dim2])/self.divDimension)) # new dimensions for image when divided\n","      xSelect,ySelect = int(np.floor(self.block/self.divDimension)),(self.block%self.divDimension) # finds coordinates for the block\n","\n","      timelapse_phase_images = [(np.asarray(tmp_timelapse.get_frame_2D(t=i,c=BFIndex))[((xSelect*newDim)+self.trim):((xSelect+1)*newDim),((ySelect*newDim)):((ySelect+1)*newDim)]).astype('uint16') for i in frameRange] # load timelapse brightfield images for block and selected frames\n","      timelapse_nuclear_images = [(np.asarray(tmp_timelapse.get_frame_2D(t=i,c=nuclearIndex))[((xSelect*newDim)+self.trim):((xSelect+1)*newDim),((ySelect*newDim)):((ySelect+1)*newDim)]).astype('uint16') for i in frameRange] # load timelapse nuclear images\n","      ipdb.set_trace()\n","      if (modelCARE!=None): # denoise nuclear images with the loaded CARE model\n","        timelapse_nuclearRestore_images = [modelCARE.predict(timelapse_nuclear_images[i], axes='YX').astype('uint16') for i in range(self.num_frames)] \n","      \n","      finalPhaseTransform = [] # list of translated brightfield timelapse images\n","      finalFluorTransform = [] # list of translated denoised nuclear fluorescent timelapse images\n","      finalRawFluorTransform = [] # list of translated raw nuclear fluorescent timelapse images\n","      displaceVec = [np.array([0.,0.])]\n","\n","      for am in range(self.num_frames-1): # for each timelapse frame except the last one\n","        tmpDisp = generateDisplacement(timelapse_phase_images[am],timelapse_phase_images[am+1]) # gets displacement between consecutive images\n","        displaceVec.append(tmpDisp) # adds the displacement to list\n","\n","      minShift_0 = np.min(np.stack( displaceVec, axis=1 ).T[:,0]) # minimum y-shift\n","      maxShift_0 = np.max(np.stack( displaceVec, axis=1 ).T[:,0]) # maximum y-shift\n","      minShift_1 = np.min(np.stack( displaceVec, axis=1 ).T[:,1]) # minimum x-shift\n","      maxShift_1 = np.max(np.stack( displaceVec, axis=1 ).T[:,1]) # maximum x-shift\n","\n","      for am in range(self.num_frames): # for each timelapse frame, translate brightfield images\n","        tform = transform.SimilarityTransform(translation=displaceVec[am])\n","        warped = transform.warp(timelapse_phase_images[am], tform, preserve_range=True, clip=True) # translate the image according to displacement vector\n","        fin = warped[int(np.abs(minShift_1)):(np.shape(warped)[1]-int(maxShift_1)),\n","                    int(np.abs(minShift_0)):(np.shape(warped)[0]-int(maxShift_0))] # clip image so all frames are the same size\n","        finalPhaseTransform.append(fin) # add image to image list\n","\n","      for am in range(self.num_frames): # for each timelapse frame, translate nuclear fluorescent images\n","        tform = transform.SimilarityTransform(translation=displaceVec[am])\n","        warped = transform.warp(timelapse_nuclearRestore_images[am], tform, preserve_range=True, clip=True)\n","        fin = warped[int(np.abs(minShift_1)):(np.shape(warped)[1]-int(maxShift_1)),\n","                    int(np.abs(minShift_0)):(np.shape(warped)[0]-int(maxShift_0))]\n","        finalFluorTransform.append(fin)\n","\n","      for am in range(self.num_frames): # for each timelapse frame, translate raw nuclear fluorescent images\n","        tform = transform.SimilarityTransform(translation=displaceVec[am])\n","        warped = transform.warp(timelapse_nuclear_images[am], tform, preserve_range=True, clip=True)\n","        fin = warped[int(np.abs(minShift_1)):(np.shape(warped)[1]-int(maxShift_1)),\n","                    int(np.abs(minShift_0)):(np.shape(warped)[0]-int(maxShift_0))]\n","        finalRawFluorTransform.append(fin)\n","\n","      self.timelapse_phase_images = finalPhaseTransform\n","      self.timelapse_nuclear_images = finalFluorTransform\n","      self.timelapse_raw_nuclear_images = finalRawFluorTransform\n","\n","      # save images to output path\n","      if not os.path.exists(self.outPath+\"/timelapse/bf/\"):\n","        os.makedirs(self.outPath+\"/timelapse/bf/\")\n","      [io.imsave(self.outPath+\"/timelapse/bf/\"+str(ind)+\".tif\",self.timelapse_phase_images[ind]) for ind in range(len(self.timelapse_phase_images))]\n","      if not os.path.exists(self.outPath+\"/timelapse/nuc/\"):\n","        os.makedirs(self.outPath+\"/timelapse/nuc/\")\n","      [io.imsave(self.outPath+\"/timelapse/nuc/\"+str(ind)+\".tif\",self.timelapse_nuclear_images[ind]) for ind in range(len(self.timelapse_nuclear_images))]\n","      if not os.path.exists(self.outPath+\"/timelapse/nuc_raw/\"):\n","        os.makedirs(self.outPath+\"/timelapse/nuc_raw/\")\n","      [io.imsave(self.outPath+\"/timelapse/nuc_raw/\"+str(ind)+\".tif\",self.timelapse_raw_nuclear_images[ind]) for ind in range(len(self.timelapse_raw_nuclear_images))]\n","\n","    else: # previously run/not forcing to rerun\n","      listFiles = os.listdir(self.outPath+\"/timelapse/bf/\")\n","      self.num_frames = len(listFiles) # update field based on how many frames were run\n","      self.timelapse_phase_images = [io.imread(self.outPath+\"/timelapse/bf/\"+listFiles[ind]) for ind in range(self.num_frames)] # read in brightfield timelapse images\n","      self.timelapse_nuclear_images = [io.imread(self.outPath+\"/timelapse/nuc/\"+listFiles[ind]) for ind in range(self.num_frames)] # read in nuclear fluorescent timelapse images\n","      self.timelapse_raw_nuclear_images = [io.imread(self.outPath+\"/timelapse/nuc_raw/\"+listFiles[ind]) for ind in range(self.num_frames)] # read in raw nuclear fluorescent timelapse images\n","\n","  def getNuclearMasks(self, modelCellpose=None, useGPU=True, flowThreshold=0.5, cellProbabilityThreshold= 1.0, nuclearDiameter = 21.0, forceReload=False):\n","    \"\"\" Segment the fluorescent nucleus using Cellpose.\n","\n","    Parameters\n","    __________\n","    modelCellpose : string, default = None\n","      Cellpose model to use, specified above\n","    useGPU : bool, default = True\n","      whether to use GPU\n","    flowThreshold : float, default = 0.5\n","      flow error threshold for cellpose\n","    cellProbabilityThreshold : float, default = 1.0\n","      DESCRIPTION HERE\n","    nuclearDiameter : float, default = 21.0\n","      average nuclear diameter for cellpose\n","    forceReload : bool, default = False\n","      whether to rerun the function\n","    \"\"\"\n","    if (not os.path.exists(self.outPath+\"/timelapse/masks/\") or forceReload): # if not run before or rerunning\n","      if not os.path.exists(self.outPath+\"/timelapse/masks/\"):\n","        os.makedirs(self.outPath+\"/timelapse/masks/\")\n","      if (len(self.timelapse_nuclear_images)>0):\n","        self.timelapse_nuclear_masks = segmentNuclei(self.timelapse_nuclear_images, modelCellpose=modelCellpose, useGPU = useGPU, flowThreshold= flowThreshold, cellProbabilityThreshold= cellProbabilityThreshold,nuclearDiameter= nuclearDiameter, channels= [0,0]) # segment using cellpose\n","        [io.imsave(self.outPath+\"/timelapse/masks/\"+str(ind)+\".tif\",self.timelapse_nuclear_masks[ind]) for ind in range(len(self.timelapse_nuclear_masks))] # save the masks\n","    else:\n","      listFiles = os.listdir(self.outPath+\"/timelapse/masks/\")\n","      self.timelapse_nuclear_masks = [io.imread(self.outPath+\"/timelapse/masks/\"+listFiles[ind]) for ind in range(self.num_frames)]\n"," \n","  def getNuclearFeatures(self, getExtraFeatures=False,extraFeaturesMaxDim=150):\n","    \"\"\"Measure various features from the segmentation masks for each frame.\n","\n","    Parameters\n","    __________\n","    getExtraFeatures : bool, default = False\n","      whether to calculate extra features (zernike, haralick, HOG)\n","    extraFeaturesMaxDim : int, default = 150\n","      size of padded image if calculating extra features\n","    \"\"\"\n","    # calculate features for each timelapse frame from the nuclear fluorescent and brightfield images\n","    self.timelapse_nucFluor_features = [collectObjectTraits(segmentationsMask = self.timelapse_nuclear_masks[i], channelImage = self.timelapse_nuclear_images[i], prefix = '', addExtras = getExtraFeatures, addNeighbors = True, maxsize = extraFeaturesMaxDim) for i in range(self.num_frames)]\n","    self.timelapse_nucPhase_features = [collectObjectTraits(segmentationsMask = self.timelapse_nuclear_masks[i], channelImage = self.timelapse_phase_images[i], prefix = 'phase_', addExtras = getExtraFeatures, maxsize = extraFeaturesMaxDim) for i in range(self.num_frames)]\n","\n","  def getTimeLapseDisplacements(self):\n","    \"\"\"Calculate the displacements between frames of timelapse nuclear images.\n","    \"\"\"\n","    self.timelapse_displacements = [generateDisplacement(self.timelapse_nuclear_images[i],self.timelapse_nuclear_images[i+1]) for i in range(self.num_frames-1)]\n","\n","\n","  def getTimeLapseConnections(self,forceReload=False):\n","    \"\"\"Determine the connections between nuclear masks in different frames.\n","\n","    Parameters\n","    __________\n","    forceReload: bool, default = False\n","      whether to rerun\n","    \"\"\"\n","    if (not os.path.exists(self.outPath+\"/timelapse/connect/\") or forceReload): # if running for first time or rereunning\n","      if not os.path.exists(self.outPath+\"/timelapse/connect/\"):\n","        os.makedirs(self.outPath+\"/timelapse/connect/\")\n","      self.timelapse_connections = [generateConnections(self.timelapse_nucFluor_features[i], self.timelapse_nucFluor_features[i+1], self.timelapse_displacements[i], i) for i in range(self.num_frames-1)] # find connections between all timepoints\n","      [self.timelapse_connections[ind].to_csv(self.outPath+\"/timelapse/connect/\"+str(ind)+\".csv\") for ind in range(len(self.timelapse_connections))] # saves connections\n","    \n","    else:\n","      listFiles = os.listdir(self.outPath+\"/timelapse/connect/\")\n","      self.timelapse_connections = [pd.read_csv(self.outPath+\"/timelapse/connect/\"+ind) for ind in listFiles]\n","    tmpList = [RLGwrap(finalConnections=self.timelapse_connections[i],\n","              FeatureFrame_t0=self.timelapse_nucFluor_features[i],\n","              FeatureFrame_t1=self.timelapse_nucFluor_features[i+1],\n","              time0=i) for i in range(self.num_frames-1)] # puts info for all connections in one dataframe\n","    blank = pd.DataFrame()\n","    self.timelapse_rlg = blank.append(tmpList)\n","    self.timelapse_rlg.to_csv(self.outPath+\"/timelapse/\"+self.sample+\"_rlg.csv\")\n","\n","  def matchTimelapseToHCR(self,forceReload=False):\n","    \"\"\"Match nuclei from the final timelapse frame with the first round HCR image.\n","\n","    Parameters\n","    __________\n","    forceReload: bool, default = False\n","      whether to rerun\n","    \"\"\"\n","    if (not os.path.exists(self.outPath+\"/hcr/rd1/raw/\") or forceReload): # if first time or rerunning\n","      if not os.path.exists(self.outPath+\"/hcr/rd1/raw/\"):\n","        os.makedirs(self.outPath+\"/hcr/rd1/raw/\")\n","      tmp_HCR1 = nd2reader.reader.ND2Reader(self.HCR1file) # read in first round HCR\n","      self.HCR_channelList = np.asarray(tmp_HCR1.metadata['channels']) # list of channels from HCR\n","      HCR1_A594 = np.asarray(tmp_HCR1.get_frame_2D(c=np.where(self.HCR_channelList=='A594')[0][0])) # get nuclear fluorescent image\n","      TL_final_A594 = self.timelapse_nuclear_images[self.num_frames-1] # get final timelapse nuclear fluorescent image\n","      self.TLtoHCR1_coords, self.TLtoHCR1_crosscor = templateCall(TL_final_A594,HCR1_A594) # finds where the timelapse image matches with the HCR image\n","      if self.TLtoHCR1_crosscor < 0.5:\n","          raise Exception(\"Global cross-correlation between TL and HCR Rd1 is too low (< 0.5)\")\n","      self.HCR_Rd1_images = [np.asarray(tmp_HCR1.get_frame_2D(c=chan))[self.TLtoHCR1_coords[0][0]:self.TLtoHCR1_coords[0][1],self.TLtoHCR1_coords[1][0]:self.TLtoHCR1_coords[1][1]] for chan in range(len(self.HCR_channelList))] # shifts the HCR images accordingly\n","\n","      np.savetxt(self.outPath+\"/hcr/channelList.txt\", self.HCR_channelList, fmt='%s') # write out the list of HCR channels\n","      [io.imsave(self.outPath+\"/hcr/rd1/raw/\"+str(ind)+\".tif\",self.HCR_Rd1_images[ind]) for ind in range(len(self.HCR_Rd1_images))] # save the raw (now shifted) HCR round 1 images\n","\n","    else: # if previously run, load in the images\n","      listFiles = os.listdir(self.outPath+\"/hcr/rd1/raw/\")\n","      self.HCR_channelList = np.loadtxt(self.outPath+\"/hcr/channelList.txt\",  dtype=str)\n","      self.HCR_Rd1_images = [io.imread(self.outPath+\"/hcr/rd1/raw/\"+ ind) for ind in listFiles]\n","\n","\n","  def matchHCRRounds(self,forceReload=False):\n","    \"\"\"Match nuclei between the two HCR rounds.\n","\n","    Parameters\n","    __________\n","    forceReload: bool, default = False\n","      whether to rerun\n","    \"\"\"\n","    if (not os.path.exists(self.outPath+\"/hcr/rd2/raw/\") or forceReload): # if first time or rerunning\n","      if not os.path.exists(self.outPath+\"/hcr/rd2/raw/\"):\n","        os.makedirs(self.outPath+\"/hcr/rd2/raw/\")\n","      tmp_HCR2 =  nd2reader.reader.ND2Reader(self.HCR2file) # read in second round HCR\n","      tmp_HCR2_DAPI = tmp_HCR2.get_frame_2D(c=np.where(self.HCR_channelList=='DAPI')[0][0]) # get DAPI image from HCR round 2\n","      self.HCR1toHCR2_coords, self.HCR1toHCR2_crosscor = templateCall(self.HCR_Rd1_images[np.where(self.HCR_channelList=='DAPI')[0][0]],tmp_HCR2_DAPI) # match to the new HCR round 1 DAPI\n","      if self.HCR1toHCR2_crosscor < 0.5:\n","          raise Exception(\"Global cross-correlation between HCR Rd1 and HCR Rd2 is too low (< 0.5)\")\n","      self.HCR_Rd2_images = [np.asarray(tmp_HCR2.get_frame_2D(c=chan))[self.HCR1toHCR2_coords[0][0]:self.HCR1toHCR2_coords[0][1],self.HCR1toHCR2_coords[1][0]:self.HCR1toHCR2_coords[1][1]] for chan in range(len(self.HCR_channelList))] # shift the HCR round 2 images accordingly\n","      [io.imsave(self.outPath+\"/hcr/rd2/raw/\"+str(ind)+\".tif\",self.HCR_Rd2_images[ind]) for ind in range(len(self.HCR_Rd2_images))]\n","    else: # load images in if already run\n","      listFiles = os.listdir(self.outPath+\"/hcr/rd2/raw/\")\n","      self.HCR_Rd2_images = [io.imread(self.outPath+\"/hcr/rd2/raw/\"+ ind) for ind in listFiles]\n","\n","  def getPostFixMasks(self,useGPU=True, flowThreshold=0.5, cellProbabilityThreshold= 1.0, nuclearDiameter = None,forceReload=False):\n","    \"\"\" Run cellpose to segment the DAPI nuclei.\n","\n","    Parameters\n","    __________\n","    useGPU : bool, default = True\n","      whether to use GPU\n","    flowThreshold : float, default = 0.5\n","      flow error threshold for cellpose\n","    cellProbabilityThreshold : float, default = 1.0\n","      DESCRIPTION HERE\n","    nuclearDiameter : float, default = None\n","      average nuclear diameter for cellpose\n","    forceReload : bool, default = False\n","      whether to rerun the function\n","    \"\"\"\n","    modelCellposeGeneral = models.Cellpose(model_type='nuclei', gpu=useGPU) # default cellpose model for nuclei\n","    \n","    if (not os.path.exists(self.outPath+\"/hcr/masks/\") or forceReload): # if not run before or rerunning\n","      if not os.path.exists(self.outPath+\"/hcr/masks/\"):\n","        os.makedirs(self.outPath+\"/hcr/masks/\")\n","      self.HCR_Rd1_mCherry_mask, flows, styles, diams = modelCellposeGeneral.eval(self.HCR_Rd1_images[np.where(self.HCR_channelList==self.nuclearChannel)[0][0]], diameter=nuclearDiameter, flow_threshold=flowThreshold, cellprob_threshold=cellProbabilityThreshold, channels=[0,0]) # get masks in mCherry for HCR round 1, save masks to object\n","      self.HCR_Rd1_DAPI_mask, flows, styles, diams = modelCellposeGeneral.eval(self.HCR_Rd1_images[np.where(self.HCR_channelList=='DAPI')[0][0]], diameter=nuclearDiameter, flow_threshold=flowThreshold, cellprob_threshold=cellProbabilityThreshold, channels=[0,0]) # get masks in DAPI for HCR round 1, save masks to object\n","      self.HCR_Rd2_DAPI_mask, flows, styles, diams = modelCellposeGeneral.eval(self.HCR_Rd2_images[np.where(self.HCR_channelList=='DAPI')[0][0]], diameter=nuclearDiameter, flow_threshold=flowThreshold, cellprob_threshold=cellProbabilityThreshold, channels=[0,0]) # get masks in DAPI for HCR round 2, save masks to object\n","\n","      # save the masks\n","      io.imsave(self.outPath+\"/hcr/masks/Rd1_mCherry_mask.tif\",self.HCR_Rd1_mCherry_mask)\n","      io.imsave(self.outPath+\"/hcr/masks/Rd1_DAPI_mask.tif\",self.HCR_Rd1_DAPI_mask)\n","      io.imsave(self.outPath+\"/hcr/masks/Rd2_DAPI_mask.tif\",self.HCR_Rd2_DAPI_mask)\n","\n","    else: # if previously run, load in the masks\n","      self.HCR_Rd1_mCherry_mask=io.imread(self.outPath+\"/hcr/masks/Rd1_mCherry_mask.tif\")\n","      self.HCR_Rd1_DAPI_mask=io.imread(self.outPath+\"/hcr/masks/Rd1_DAPI_mask.tif\")\n","      self.HCR_Rd2_DAPI_mask=io.imread(self.outPath+\"/hcr/masks/Rd2_DAPI_mask.tif\")\n","\n","  def getHCRMeasurements(self,dilationRadiusPixels = 9,forceReload=False):\n","    \"\"\" Calculate the mean, median, etc… for gene expression using the HCR fluorescence channels.\n","\n","    Parameters\n","    __________\n","    dilationRadiusPixels: int, default = 9\n","      how many pixels to dilate the radius for measuring HCR fluorescence\n","    forceReload: bool, default = False\n","      whether to rerun\n","    \"\"\"\n","    HCRmeas_filename = self.outPath+\"/hcr/\"+self.sample + \"_HCRmeasures_dil\" + str(dilationRadiusPixels) + \".csv\"\n","    if (not os.path.isfile(HCRmeas_filename) or forceReload): # if running for the first time or rerunning\n","      HCR_Rd1_dilationMask,HCR_Rd1_translationTable = getDilatedMasks(self.HCR_Rd1_DAPI_mask,round=\"Rd1\",dilationPixels = dilationRadiusPixels) # get dilated masks and labels for round 1\n","      HCR_Rd2_dilationMask,HCR_Rd2_translationTable = getDilatedMasks(self.HCR_Rd2_DAPI_mask,round=\"Rd2\",dilationPixels = dilationRadiusPixels) #round 2\n","\n","      # calculate intensities for each channel and merge into one dataframe\n","      # HCR_Rd1_measures = [measureIntProps(self.HCR_Rd1_DAPI_mask,self.HCR_channelList[i],channelImage = self.HCR_Rd1_images[i]) for i in range(len(self.HCR_channelList))]\n","      # HCR_Rd1_measures = reduce(lambda x, y: pd.merge(x, y, on = ['label','centroid-0','centroid-1']), HCR_Rd1_measures)\n","\n","      # HCR_Rd2_measures = [measureIntProps(self.HCR_Rd2_DAPI_mask,self.HCR_channelList[i],channelImage = self.HCR_Rd2_images[i]) for i in range(len(self.HCR_channelList))]\n","      # HCR_Rd2_measures = reduce(lambda x, y: pd.merge(x, y, on = ['label','centroid-0','centroid-1']), HCR_Rd2_measures)\n","      \n","      # the properties of the laplacian of the image, using dilated masks\n","      HCR_Rd1_measures_lp = [measureIntProps(HCR_Rd1_dilationMask,self.HCR_channelList[i],channelImage = np.clip(filters.laplace(self.HCR_Rd1_images[i], ksize=15, mask=None), 0, None)) for i in range(len(self.HCR_channelList))]\n","      HCR_Rd1_measures_lp = reduce(lambda x, y: pd.merge(x, y, on = ['label','centroid-0','centroid-1']), HCR_Rd1_measures_lp)\n","\n","      HCR_Rd2_measures_lp = [measureIntProps(HCR_Rd2_dilationMask,self.HCR_channelList[i],channelImage = np.clip(filters.laplace(self.HCR_Rd2_images[i], ksize=15, mask=None), 0, None)) for i in range(len(self.HCR_channelList))]\n","      HCR_Rd2_measures_lp = reduce(lambda x, y: pd.merge(x, y, on = ['label','centroid-0','centroid-1']), HCR_Rd2_measures_lp)\n","\n","      # dilated mask properties\n","      HCR_Rd1_measures_dil = [measureIntProps(HCR_Rd1_dilationMask,self.HCR_channelList[i],channelImage = self.HCR_Rd1_images[i]) for i in range(len(self.HCR_channelList))]\n","      HCR_Rd1_measures_dil = reduce(lambda x, y: pd.merge(x, y, on = ['label','centroid-0','centroid-1']), HCR_Rd1_measures_dil)\n","\n","      HCR_Rd2_measures_dil = [measureIntProps(HCR_Rd2_dilationMask,self.HCR_channelList[i],channelImage = self.HCR_Rd2_images[i]) for i in range(len(self.HCR_channelList))]\n","      HCR_Rd2_measures_dil = reduce(lambda x, y: pd.merge(x, y, on = ['label','centroid-0','centroid-1']), HCR_Rd2_measures_dil)\n","\n","      # calculate mask features based on DAPI - not dilated\n","      HCR_Rd1_DAPImask_props = pd.DataFrame(measure.regionprops_table(self.HCR_Rd1_DAPI_mask,intensity_image= self.HCR_Rd1_images[np.where(self.HCR_channelList=='DAPI')[0][0]], properties=['label','bbox','centroid','area','mean_intensity'], extra_properties=(quantiles,)))\n","      HCR_Rd2_DAPImask_props = pd.DataFrame(measure.regionprops_table(self.HCR_Rd2_DAPI_mask,intensity_image= self.HCR_Rd2_images[np.where(self.HCR_channelList=='DAPI')[0][0]], properties=['label','bbox','centroid','area','mean_intensity'], extra_properties=(quantiles,)))\n","\n","      # connect the two HCR rounds by their DAPI images\n","      displaced = generateDisplacement(self.HCR_Rd1_images[np.where(self.HCR_channelList=='DAPI')[0][0]],self.HCR_Rd2_images[np.where(self.HCR_channelList=='DAPI')[0][0]]) # get displacements\n","      HCR_connections = generateConnections(HCR_Rd1_DAPImask_props, HCR_Rd2_DAPImask_props, displaced, time=0,allowSplitting=False) # find the connections\n","      HCR_connections = HCR_connections[HCR_connections['annotation'] == 'pass'][['start','end']]\n","      HCR_connections = HCR_connections.rename(columns={'start':'Rd1_orig_label','end': 'Rd2_orig_label'})\n","      # HCR_connections is merged to contain the original and dilated labels for each nucleus connected between Rd1 and Rd2\n","      HCR_connections = pd.merge(pd.merge(HCR_connections,HCR_Rd1_translationTable,on ='Rd1_orig_label'),HCR_Rd2_translationTable,on ='Rd2_orig_label') # merge HCR labels into one dataframe (dilation, connection, both rounds)\n","      # rename columns so they can be merged properly\n","      # properties of the laplacian image are prefixed with lp\n","      HCR_Rd1_measures_lp = HCR_Rd1_measures_lp.add_prefix(\"lp_Rd1_\")\n","      HCR_Rd1_measures_lp = HCR_Rd1_measures_lp.rename(columns={'lp_Rd1_label':'Rd1_dilated_label'})\n","      HCR_Rd2_measures_lp = HCR_Rd2_measures_lp.add_prefix(\"lp_Rd2_\")\n","      HCR_Rd2_measures_lp = HCR_Rd2_measures_lp.rename(columns={'lp_Rd2_label':'Rd2_dilated_label'})\n","      # properties of the original (nuclear) image are prefixed with nuc - describe dilated nucleus\n","      HCR_Rd1_measures_dil = HCR_Rd1_measures_dil.add_prefix(\"nuc_Rd1_\")\n","      HCR_Rd1_measures_dil = HCR_Rd1_measures_dil.rename(columns={'nuc_Rd1_label':'Rd1_dilated_label'})\n","      HCR_Rd2_measures_dil = HCR_Rd2_measures_dil.add_prefix(\"nuc_Rd2_\")\n","      HCR_Rd2_measures_dil = HCR_Rd2_measures_dil.rename(columns={'nuc_Rd2_label':'Rd2_dilated_label'})\n","\n","      # merge all the measurements\n","      self.HCR_measurements = pd.merge(HCR_connections, HCR_Rd1_measures_lp, on = 'Rd1_dilated_label')\n","      self.HCR_measurements = pd.merge(self.HCR_measurements, HCR_Rd2_measures_lp, on = 'Rd2_dilated_label')\n","      self.HCR_measurements = pd.merge(self.HCR_measurements, HCR_Rd1_measures_dil, on = 'Rd1_dilated_label')\n","      self.HCR_measurements = pd.merge(self.HCR_measurements, HCR_Rd2_measures_dil, on = 'Rd2_dilated_label')\n","\n","      # rename label columns to indicate they are labels of the DAPI image\n","      self.HCR_measurements = self.HCR_measurements.rename(columns={'Rd1_orig_label':'Rd1_orig_label_DAPI', 'Rd1_dilated_label':'Rd1_dilated_label_DAPI', 'Rd2_orig_label':'Rd2_orig_label_DAPI', 'Rd2_dilated_label':'Rd2_dilated_label_DAPI'})\n","\n","      self.HCR_measurements = getCellTypes(self.HCR_measurements, clu_thresh = 0.006) # annotate the cell types\n","      \n","      self.HCR_measurements.to_csv(HCRmeas_filename) # save intensity data as a csv\n","    else:\n","      self.HCR_measurements = pd.read_csv(HCRmeas_filename)    \n","\n","  def connectTimeLapseHCR(self,allowDivision=True,HCR_DAPIsub=False,withCare=True, modelCARE=None, modelCellpose=None, diameter=21.0, flow_threshold=0.5, cellprob_threshold=1.0,forceReload=False, local_xcor=True, xcor_thresh = 0.9):\n","    \"\"\"Connect the final timelapse masks to their counterpart in the HCR image.\n","\n","    Parameters\n","    __________\n","    allowDivision: bool, default = True\n","      whether to allow for cell divisions between last timelapse and HCR Rd 1\n","    HCR_DAPIsub: bool, default = False\n","      whether to use DAPI masks or mCherry masks to connect to timelapse\n","    withCare: bool, default = True\n","      whether to restore HCR mCherry images before finding masks, only needed if HCR_DAPIsub = False\n","    modelCARE: str, default = None\n","      CARE model as defined, only needed if HCR_DAPIsub = False\n","    modelCellpose: str, default = None\n","      Cellpose model as defined, only needed if HCR_DAPIsub = False\n","    diameter: float, default = None\n","      average nuclear diameter for cellpose\n","    flowThreshold : float, default = 0.5\n","      flow error threshold for cellpose\n","    cellProbabilityThreshold : float, default = 1.0\n","      DESCRIPTION HERE\n","    forceReload: bool, default = False\n","      whether to rerun\n","    local_xcor: bool, default=True\n","      if local cross-correlation should be used to make connections between timelapse and HCR (if False, uses connection algorithm)\n","    xcor_thresh: float, default = 0.9\n","      threshold for whether a last timelapse nucleus has a match in the HCR Rd1 mCherry image based on local cross-correlation\n","    \"\"\"\n","    finalIDs_filename = self.outPath+\"/hcr/\"+self.sample + \"_finalIDs.csv\"\n","    TLtoHCR_filename = self.outPath+\"/hcr/\"+self.sample + \"_TLtoHCR.csv\"   \n","    if (not os.path.isfile(finalIDs_filename) or forceReload): # if running for first time or rerunning\n","      HCR_Rd1_mCherry_image = (self.HCR_Rd1_images[np.where(self.HCR_channelList==self.nuclearChannel)[0][0]]) # get the HCR rd1 mCherry image\n","      if (HCR_DAPIsub):\n","        HCR_Rd1_mCherry_mask = self.HCR_Rd1_DAPI_mask # use DAPI mask if available\n","      else:\n","        if (withCare): # restore HCR Rd1 mCherry images with CARE and find masks\n","          HCR_Rd1_mCherry_image = modelCARE.predict(HCR_Rd1_mCherry_image, axes='YX').astype('uint16')\n","          self.HCR_Rd1_mCherry_image = HCR_Rd1_mCherry_image\n","          HCR_Rd1_mCherry_mask, flows, styles = modelCellpose.eval(HCR_Rd1_mCherry_image, diameter=diameter, flow_threshold=flow_threshold, cellprob_threshold=cellprob_threshold, channels=[0,0])\n","          self.HCR_Rd1_mCherry_mask = HCR_Rd1_mCherry_mask\n","        else: # find masks of HCR Rd1 mCherry images\n","          modelCellposeGeneral = models.Cellpose(model_type='nuclei', gpu=True)\n","          HCR_Rd1_mCherry_mask, flows, styles, diams = modelCellposeGeneral.eval(HCR_Rd1_mCherry_image, diameter=diameter, flow_threshold=flow_threshold, cellprob_threshold=cellprob_threshold, channels=[0,0])\n","\n","      TL_last_mCherry_mask = (self.timelapse_nuclear_masks[(self.num_frames-1)]) # get the masks from the last image\n","      TL_last_mCherry_image = (self.timelapse_nuclear_images[(self.num_frames-1)]) # get the last image\n","      displaced = generateDisplacement(TL_last_mCherry_image,HCR_Rd1_mCherry_image) # find displacement (cross correlation)\n","\n","      # get the mCherry mask properties for the last timelapse and the first round HCR\n","      TL_last_mCherrymask_props = pd.DataFrame(measure.regionprops_table(TL_last_mCherry_mask,intensity_image= TL_last_mCherry_image, \n","                                                                        properties=['label','bbox','centroid','area','mean_intensity']))\n","      HCR_Rd1_mCherrymask_props = pd.DataFrame(measure.regionprops_table(HCR_Rd1_mCherry_mask,intensity_image= HCR_Rd1_mCherry_image, \n","                                                                        properties=['label','bbox','centroid','area','mean_intensity']))\n","      # translation between mCherry and DAPI nuclei\n","      mCherry_DAPI_translation = mCherryToDAPITranslation(HCR_Rd1_mCherrymask_props, self.HCR_measurements)\n","\n","\n","      # connect timelapse to first round of HCR\n","      if local_xcor: # using local xcor method\n","        TLtoHCR_connections = connectWithLocalXCor(displaced, self.num_frames, TL_last_mCherrymask_props, HCR_Rd1_mCherry_image, TL_last_mCherry_image, TL_last_mCherry_mask, HCR_Rd1_mCherry_mask, xcor_threshold = xcor_thresh)\n","      else: # using original connection algo\n","        TLtoHCR_connections = generateConnectionsFixed(TL_last_mCherrymask_props, HCR_Rd1_mCherrymask_props, displaced, time=0,allowSplitting=allowDivision)\n","        TLtoHCR_connections['MasterID_'+str(self.num_frames-1)] = str(self.num_frames-1)+\"_\"+TLtoHCR_connections['start'].astype(str)\n","        TLtoHCR_connections['Rd1_orig_label'] = TLtoHCR_connections['end']\n","        TLtoHCR_connections = TLtoHCR_connections[['MasterID_'+str(self.num_frames-1),'Rd1_orig_label']]\n","\n","      # matching labels, regardless of method\n","      TLtoHCR_connections.columns = ['MasterID_' + str(self.num_frames - 1), 'Rd1_orig_label']\n","      TLtoHCR_connections.drop_duplicates('Rd1_orig_label', keep = False, inplace = True)\n","      matching = [self.timelapse_connections[i][self.timelapse_connections[i].annotation.isin(['pass','split'])].filter(like='Master', axis=1) for i in range(self.num_frames-1)] # just take the columns with the master IDs from each timelapse\n","      matching.append(TLtoHCR_connections) # append the TL to HCR connection labels\n","      matching.append(mCherry_DAPI_translation) # append the HCR mCherry to HCR DAPI labels\n","      matching.append(self.HCR_measurements.filter(like='label', axis=1)) # append the HCR DAPI labels (original and dilated)\n","      self.finalIDs = reduce(lambda x, y: pd.merge(x, y), matching) # put all the labels into one dataframe\n","      self.TLtoHCR_connections = TLtoHCR_connections # save the timelapse-HCR connections\n","      self.finalIDs.to_csv(finalIDs_filename) # write out the IDs\n","      self.TLtoHCR_connections.to_csv(TLtoHCR_filename) # write out the connections\n","\n","      self.finalIDs = pd.read_csv(finalIDs_filename)\n","      self.TLtoHCR_connections = pd.read_csv(TLtoHCR_filename)\n","\n","      self.masterDF = createMasterDataframe(self.finalIDs, self.num_frames, self.timelapse_connections, self.timelapse_rlg, self.HCR_measurements, HCR_Rd1_mCherrymask_props)\n","      self.masterDF.to_csv(self.outPath + '/' + self.sample + '_masterDF.csv')\n","      print(\"done\")\n","\n","  def saveDataStructure(self):\n","    \"\"\" Save the FateTrack object as a pkl file.\"\"\"\n","    save_object(self, self.outPath+'/'+self.sample+'.pkl')\n","    \n","  def saveMetaData(self):\n","    \"\"\" Save metadata for the FateTrack object.\"\"\"\n","    with open(self.outPath+'/'+self.sample+'_metadata.txt', 'w') as fh:\n","      fh.write(f'sample : {self.sample}\\n')\n","      fh.write(f'path : {self.outPath}\\n')\n","      fh.write(f'file timelapse : {self.TLfile}\\n')\n","      fh.write(f'file HCR_Rd1 : {self.HCR1file}\\n')\n","      fh.write(f'file HCR_Rd2 : {self.HCR2file}\\n')\n","      fh.write(f'number frames : {self.num_frames}\\n')\n","      fh.write(f'trailing frames : {self.keepTrailingFrames}\\n')\n","      fh.write(f'total image blocks : {self.divDimension**2}\\n')\n","      fh.write(f'numbered image block : {self.block}\\n')\n","      fh.write(f'crop_from_top_timelapse : {self.trim}\\n')\n","      for i in range(len(self.HCR_channelList)):\n","        fh.write(f'HCR channel #{i} : {self.HCR_channelList[i]}\\n')\n","      #fh.write(f'TLtoHCR1_crosscor : {self.TLtoHCR1_crosscor}\\n')\n","      #fh.write(f'HCR1toHCR2_crosscor : {self.HCR1toHCR2_crosscor}\\n')\n","\n","      now = datetime.now()\n","      dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\") \n","      fh.write(f'date/time processed : {dt_string}\\n')\n","      fh.write(f'CARE model : {modelNameCARE}/{modelPathCARE}\\n')\n","      fh.write(f'Cellpose model : {modelCellpose}\\n')\n","      fh.write(f'Cellpose, mean diam. : {meanNuclearDiameter}\\n')\n","      fh.write(f'Cellpose, use Torch : {use_torch}\\n')\n","      fh.write(f'Cellpose, use GPU : {use_GPU}\\n')\n","      fh.write(f'Cellpose, use net. avg. : {use_network_averaging}\\n')\n","    fh.close()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2IlGBFts697m"},"source":["### Set parameters and load relevant ML models \n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"elapsed":194,"status":"error","timestamp":1669933181910,"user":{"displayName":"Fate Track","userId":"00899138229569203583"},"user_tz":300},"id":"gEb8eDs1nRS1","outputId":"314c53a3-8bbd-42be-fb37-81fd1fc503ab"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-674518f80017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodelNameCARE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"20210610_test_rollingball_lowsnr\"\u001b[0m \u001b[0;31m#@param {type:\"string\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodelPathCARE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/FateTrack_Main/FateTrack_master/Denoise_20210610\"\u001b[0m \u001b[0;31m#@param {type:\"string\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodelCAREmCherry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCARE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodelNameCARE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasedir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodelPathCARE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#@markdown #### Load relevant model for  Cellpose (segmenting images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'CARE' is not defined"]}],"source":["#@markdown #### Load relevant model for CARE (denoising images) \n","# load CARE model\n","#default modelNameCARE='20210610_test_rollingball_lowsnr', modelPathCARE='/content/drive/MyDrive/Denoise_20210610'\n","modelNameCARE = \"20210610_test_rollingball_lowsnr\" #@param {type:\"string\"}\n","modelPathCARE = \"/content/drive/MyDrive/FateTrack_Main/FateTrack_master/Denoise_20210610\" #@param {type:\"string\"}\n","modelCAREmCherry = CARE(config=None, name=modelNameCARE, basedir=modelPathCARE)\n","\n","#@markdown #### Load relevant model for  Cellpose (segmenting images)\n","#default modelCellpose = \"/content/gdrive/MyDrive/FateTrackwithDenoise/cellpose_own_20210622/training/models/content/gdrive/MyDrive/FateTrackwithDenoise/cellpose_own_20210622/training/models/cellpose_20210623\"\n","# load Cellpose model\n","modelCellpose = \"/content/gdrive/MyDrive/FateTrack_Main/FateTrack_master/FateTrackwithDenoise/cellpose_own_20210622/training/models/content/gdrive/MyDrive/FateTrackwithDenoise/cellpose_own_20210622/training/models/cellpose_20210623\" #@param {type:\"string\"}\n","meanNuclearDiameter=17.0#@param {type:\"number\"}\n","use_torch=True #@param {type:\"boolean\"}\n","use_GPU=True #@param {type:\"boolean\"}\n","use_network_averaging=True#@param {type:\"boolean\"}\n","modelCellposeMCherry = models.CellposeModel(gpu=use_GPU, pretrained_model=modelCellpose, \n","                                            # torch=use_torch, \n","                                            diam_mean=meanNuclearDiameter,\n","                                            net_avg=use_network_averaging,device=None, \n","                                            residual_on=True, style_on=True,concatenation=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CJ2_GLjG2bg"},"outputs":[],"source":["#@markdown ### Define parameters\n","#@markdown ###### Provide sample name, files, and output folder.\n","#@markdown ###### Set number of trailing frames to keep (i.e. to keep remaining 9 frames, set to 9).\n","#@markdown ###### To keep all frames, set to \"all\" (this is the default).\n","keep_trailing_frames = \"2\" #@param {type:\"string\"}\n","sample_base_name = \"507_D3_1_w1\" #@param {type:\"string\"}\n","#@markdown ###### Provide total number of blocks (equal divisions of the timelapse image, must be a square number).\n","#@markdown ###### To run all blocks, starting division should be 0 and ending division should equal the number of divisions.\n","number_of_divisions =  9#@param {type:\"number\"}\n","starting_division =  0#@param {type:\"number\"}\n","ending_division =  9#@param {type:\"number\"}\n","if(keep_trailing_frames == \"0\"):\n","  sample_name = sample_base_name + \"_no_trailing\"\n","else:\n","  sample_name = sample_base_name + '_' + keep_trailing_frames + \"trailing\"\n","file_timelapse = \"/content/drive/MyDrive/FateTrack_Main/Drive2/select_data/timelapse_training/\" + sample_base_name +  \"_training.nd2\"\n","file_HCR_Round1 = \"/content/drive/MyDrive/FateTrack_Main/Drive2/select_data/HCR_final_training/\" + sample_base_name + \"_HCR_Rd1.nd2\"\n","file_HCR_Round2 = \"/content/drive/MyDrive/FateTrack_Main/Drive2/select_data/HCR_final_training/\"+ sample_base_name + \"_HCR_Rd2.nd2\"\n","path_to_output = \"/content/drive/MyDrive/FateTrack_Main/test/1028\" #@param {type:\"string\"}"]},{"cell_type":"markdown","metadata":{"id":"UE2c8gquLQ1w"},"source":["## Run image processing pipeline and analyze output."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5413957,"status":"ok","timestamp":1666985512161,"user":{"displayName":"Fate Track","userId":"00899138229569203583"},"user_tz":240},"id":"ZWPYk4DjDjL6","outputId":"ede7a9e4-9d51-4826-bf11-ff1c61cd9399"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 40ms/step\n","Minimum cost: 38742\n","Minimum cost: 33119\n","1/1 [==============================] - 0s 43ms/step\n","done\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 38ms/step\n","Minimum cost: 51841\n","Minimum cost: 57240\n","1/1 [==============================] - 0s 40ms/step\n","done\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 39ms/step\n","Minimum cost: 38535\n","Minimum cost: 33172\n","1/1 [==============================] - 0s 41ms/step\n","done\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 44ms/step\n","Minimum cost: 46724\n","Minimum cost: 32159\n","1/1 [==============================] - 0s 42ms/step\n","done\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 41ms/step\n","Minimum cost: 57246\n","Minimum cost: 53150\n","1/1 [==============================] - 0s 42ms/step\n","done\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 40ms/step\n","Minimum cost: 54567\n","Minimum cost: 53751\n","1/1 [==============================] - 0s 40ms/step\n","done\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 41ms/step\n","Minimum cost: 29992\n","Minimum cost: 17234\n","1/1 [==============================] - 0s 45ms/step\n","done\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 39ms/step\n","Minimum cost: 28309\n","Minimum cost: 21909\n","1/1 [==============================] - 0s 41ms/step\n","done\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 41ms/step\n","Minimum cost: 26032\n","Minimum cost: 14960\n","1/1 [==============================] - 0s 41ms/step\n","done\n"]}],"source":["#@markdown #### Running the image processing pipeline.\n","\n","# Run in bulk\n","force_the_rerun = False\n","\n","for blockNumber in range(starting_division,ending_division):\n","  test = FateTrack(sample= sample_name+'_block'+str(blockNumber)+'of'+str(number_of_divisions),\n","                   block=blockNumber, \n","                   divDimension=int(np.sqrt(number_of_divisions)), \n","                   outPath = path_to_output,\n","                   nuclearChannel='A594',\n","                   timelapseFile =file_timelapse,\n","                   HCRFile1 = file_HCR_Round1,\n","                   HCRFile2 = file_HCR_Round2,\n","                   timelapseTopTrim = 0,keepTrailingFrames=keep_trailing_frames)\n","\n","  test.loadTimeLapse(modelCARE=modelCAREmCherry,forceReload=force_the_rerun )\n","  test.getNuclearMasks(modelCellpose=modelCellposeMCherry,forceReload=force_the_rerun)\n","  test.getNuclearFeatures()\n","  test.getTimeLapseDisplacements()\n","  test.getTimeLapseConnections(forceReload=force_the_rerun)\n","  test.matchTimelapseToHCR(forceReload=force_the_rerun)\n","  test.matchHCRRounds(forceReload=force_the_rerun)\n","  test.getPostFixMasks(forceReload=force_the_rerun)\n","  test.getHCRMeasurements(forceReload=force_the_rerun)\n","  test.connectTimeLapseHCR(modelCARE=modelCAREmCherry, modelCellpose=modelCellposeMCherry, forceReload=force_the_rerun)\n","  test.saveDataStructure()\n","  test.saveMetaData()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptHz0l6f5fP6"},"outputs":[],"source":["# Run single block\n","force_the_rerun = False\n","blockNumber = 0\n","test = FateTrack(sample= sample_name+'_block'+str(blockNumber)+'of'+str(number_of_divisions),\n","                   block=blockNumber, \n","                   divDimension=int(np.sqrt(number_of_divisions)), \n","                   outPath = path_to_output,\n","                   nuclearChannel='A594',\n","                   timelapseFile =file_timelapse,\n","                   HCRFile1 = file_HCR_Round1,\n","                   HCRFile2 = file_HCR_Round2,\n","                   timelapseTopTrim = 0,keepTrailingFrames=keep_trailing_frames)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ub3Tb1uQFCjN","outputId":"540ce20f-7a14-458c-d733-7eb7dc52005b"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","PYDEV DEBUGGER WARNING:\n","sys.settrace() should not be used when the debugger is being used.\n","This may cause the debugger to stop working correctly.\n","If this is needed, please check: \n","http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n","to see how to restore the debug tracing back correctly.\n","Call Location:\n","  File \"/usr/lib/python3.7/bdb.py\", line 332, in set_trace\n","    sys.settrace(self.trace_dispatch)\n","\n"]},{"output_type":"stream","name":"stdout","text":["> \u001b[0;32m<ipython-input-12-8b3e258ec6ba>\u001b[0m(163)\u001b[0;36mloadTimeLapse\u001b[0;34m()\u001b[0m\n","\u001b[0;32m    162 \u001b[0;31m      \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[0;32m--> 163 \u001b[0;31m      \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodelCARE\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# denoise nuclear images with the loaded CARE model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[0;32m    164 \u001b[0;31m        \u001b[0mtimelapse_nuclearRestore_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodelCARE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimelapse_nuclear_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'YX'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint16'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\n","[array([[1370, 1344, 1303, ..., 1266, 1263, 1180],\n","       [1313, 1424, 1372, ..., 1298, 1308, 1334],\n","       [1429, 1417, 1390, ..., 1264, 1295, 1274],\n","       ...,\n","       [1569, 1531, 1613, ..., 1550, 1995, 2711],\n","       [1595, 1573, 1553, ..., 1778, 2324, 3109],\n","       [1535, 1461, 1420, ..., 1790, 2739, 3519]], dtype=uint16), array([[1462, 1512, 1462, ..., 1236, 1319, 1219],\n","       [1580, 1510, 1472, ..., 1255, 1270, 1237],\n","       [1489, 1509, 1531, ..., 1298, 1287, 1254],\n","       ...,\n","       [1650, 1722, 1608, ..., 1302, 1291, 1220],\n","       [1648, 1532, 1728, ..., 1183, 1273, 1330],\n","       [1730, 1690, 1596, ..., 1363, 1285, 1276]], dtype=uint16)]\n","2\n","*** SyntaxError: unexpected EOF while parsing\n","array([[1370, 1344, 1303, ..., 1266, 1263, 1180],\n","       [1313, 1424, 1372, ..., 1298, 1308, 1334],\n","       [1429, 1417, 1390, ..., 1264, 1295, 1274],\n","       ...,\n","       [1569, 1531, 1613, ..., 1550, 1995, 2711],\n","       [1595, 1573, 1553, ..., 1778, 2324, 3109],\n","       [1535, 1461, 1420, ..., 1790, 2739, 3519]], dtype=uint16)\n","(3104, 3104)\n","--KeyboardInterrupt--\n","--KeyboardInterrupt--\n","--KeyboardInterrupt--\n","--KeyboardInterrupt--\n","--KeyboardInterrupt--\n","--KeyboardInterrupt--\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqG8a5GgE-o2"},"outputs":[],"source":["test.getNuclearMasks(modelCellpose=modelCellposeMCherry,forceReload=force_the_rerun)\n","test.getNuclearFeatures()\n","test.getTimeLapseDisplacements()\n","test.getTimeLapseConnections(forceReload=force_the_rerun)\n","test.matchTimelapseToHCR(forceReload=force_the_rerun)\n","test.matchHCRRounds(forceReload=force_the_rerun)\n","test.getPostFixMasks(forceReload=force_the_rerun)\n","test.getHCRMeasurements(forceReload=force_the_rerun)\n","test.connectTimeLapseHCR(modelCARE=modelCAREmCherry, modelCellpose=modelCellposeMCherry, forceReload=force_the_rerun)\n","test.saveDataStructure()\n","test.saveMetaData()"]},{"cell_type":"markdown","metadata":{"id":"mJD2ByHe6f4U"},"source":["# Sandbox"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obxWv1aLNfiT"},"outputs":[],"source":["# load in a FateTrack object that you have already run\n","fto = load_object('/content/drive/MyDrive/FateTrack_Main/test/1021/516_D1_1_w1_9trailing_block0of9/516_D1_1_w1_9trailing_block0of9.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SohhYxcr6WiW"},"outputs":[],"source":["#fto.outPath = path_to_output + fto.sample\n","#fto.getHCRMeasurements(forceReload = True)\n","fto.outPath = path_to_output + '/516_D1_1_w1_9trailing_block0of9'\n","fto.connectTimeLapseHCR(xcor_thresh = 0.85, local_xcor = True, HCR_DAPIsub = False, withCare = True, modelCARE = modelCAREmCherry, modelCellpose = modelCellposeMCherry, diameter = meanNuclearDiameter, forceReload = True)\n","#fto.saveDataStructure()\n","#fto.saveMetaData()"]},{"cell_type":"markdown","metadata":{"id":"WiB6yq1AKgTM"},"source":["# Plotting Connections"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjbvISweJGMw"},"outputs":[],"source":["!pip install mpld3\n","import mpld3\n","from mpld3 import plugins\n","from matplotlib import collections  as mc\n","from matplotlib import colors as clrs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meJiTFPgtYrE"},"outputs":[],"source":["sample_name_with_block = \"516_D1_1_w1_9trailing_block0of9\"\n","fto = load_object('/content/drive/MyDrive/FateTrack_Main/test/1021/' + sample_name_with_block + '/' + sample_name_with_block + '.pkl')\n","tl_mask = fto.timelapse_nuclear_masks[8]\n","hcr_mask = fto.HCR_Rd1_mCherry_mask\n","dapi_mask = fto.HCR_Rd1_DAPI_mask"]},{"cell_type":"code","source":["fto.timelapse_displacements[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkT8o9qWyXcX","executionInfo":{"status":"ok","timestamp":1669933927371,"user_tz":300,"elapsed":6,"user":{"displayName":"Fate Track","userId":"00899138229569203583"}},"outputId":"8e160e5b-468c-4332-da75-06c07c46cf42"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0.])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kins1Zv3FSM4"},"outputs":[],"source":["# create composite image\n","hcr_mask = (hcr_mask > 0) * 2;\n","comp = (tl_mask > 0) + hcr_mask;\n","\n","# with DAPI mask\n","# hcr_mask = (hcr_mask > 0) * 2\n","# tl_mask = tl_mask > 0\n","# dapi_mask = (dapi_mask > 0) * 4\n","# comp = tl_mask + hcr_mask + dapi_mask\n","# 0 = nothing (black)\n","# 1 = TL (blue)\n","# 2 = HCR_mCherry (red)\n","# 3 = TL/HCR_mCherry (magenta)\n","# 4 = HCR_DAPI (yellow)\n","# 5 = TL/HCR_DAPI (green)\n","# 6 = HCR_mCherry/HCR_DAPI (orange)\n","# 7 = TL/HCR_mCherry/HCR_DAPI (white)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vrDZafs4i_n"},"outputs":[],"source":["# plot TL and HCR Rd1 nuclei and their connections\n","\n","fig = plt.figure(figsize = (5,5))\n","ax1 = fig.add_subplot(111)\n","im = comp #plt.imread(path_to_output + '/' + sample_name_with_block + '/Composite.png')\n","ax1.imshow(im, origin = 'lower', cmap = clrs.ListedColormap(['w', 'b', 'r', 'm'])) # nothing, TL, TL/HCR_mCherry, HCR_mCherry\n","# ax1.imshow(im, origin = 'lower', cmap = clrs.ListedColormap([[0, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [0, 1, 0], [1, 0.5, 0], [1, 1, 1]])) # nothing, TL, TL/HCR_mCherry, HCR_mCherry, HCR_mCherry/HCR_DAPI, HCR_DAPI\n","\n","#ax1.scatter(connections.tl_x, connections.tl_y, s=0.1, c='g', marker=\"s\", label='timelapse')\n","\n","# lc = mc.LineCollection(lines, linewidths=1, color = 'g')\n","# ax1.add_collection(lc)\n","for i in range(0, fto.masterDF.shape[0]):\n","  ax1.plot([fto.masterDF['xCoord_Rd1_mCherry'][i], fto.masterDF['xCoord_8'][i]], [fto.masterDF['yCoord_Rd1_mCherry'][i], fto.masterDF['yCoord_8'][i]], 'g', linewidth = 0.25)\n","\n","# plt.show()\n","# mpld3.display()\n","plt.savefig(fto.outPath + \"/connections_85_larger_rad_516_D1_1_w1_b0.png\", dpi = 1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmN1YAiecffG"},"outputs":[],"source":["## adapted to test mCherry to DAPI connections in first round of HCR\n","mCherry_mask = fto.HCR_Rd1_DAPI_mask\n","DAPI_mask = fto.HCR_Rd1_mCherry_mask\n","\n","centroid_translation = np.concatenate([np.asarray(mCherry[['centroid-0', 'centroid-1']]), np.asarray(DAPI.iloc[closest_DAPI_nucleus][['nuc_Rd1_centroid-0', 'nuc_Rd1_centroid-1']])], axis = 1)\n","centroid_translation = np.delete(centroid_translation, np.nonzero(np.amin(centroid_dist, axis = 1) > 10), axis = 0)  # remove rows where distance between the nuclei > 10 px\n","\n","# mCherry_coords = zip(mCherry_centroids[:,1], mCherry_centroids[:,0])\n","# DAPI_coords = zip(DAPI_centroids[:,1], DAPI_centroids[:,0])\n","# lines = zip(mCherry_coords, DAPI_coords)\n","\n","# create composite image\n","mCherry_mask = (mCherry_mask > 0) * 2\n","comp = (DAPI_mask > 0) + mCherry_mask\n","\n","fig = plt.figure(figsize = (5,5))\n","ax1 = fig.add_subplot(111)\n","im = comp #plt.imread(path_to_output + '/' + sample_name_with_block + '/Composite.png')\n","ax1.imshow(im, origin = 'lower', cmap = clrs.ListedColormap(['w', 'b', 'r', 'm']))\n","#ax1.scatter(connections.tl_x, connections.tl_y, s=0.1, c='g', marker=\"s\", label='timelapse')\n","\n","# lc = mc.LineCollection(lines, linewidths=1, color = 'g')\n","# ax1.add_collection(lc)\n","for i in range(centroid_translation.shape[0]):\n","  ax1.plot([centroid_translation[i][1], centroid_translation[i][3]], [centroid_translation[i][0], centroid_translation[i][2]], 'g', linewidth = 0.25)\n","\n","# plt.show()\n","# mpld3.display()\n","plt.savefig(path_to_output + '/' + \"mCherry_DAPI_connections_10px.png\", dpi = 1000)\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1pcLN_wABN6x_alpzYuVOuSi1N_7kLabw","timestamp":1667073852277},{"file_id":"13fHYr_ZSgxqQjqpcoA3OpHlu4K267hiK","timestamp":1653670762512},{"file_id":"1VqVtvtTC98X18JX-uWZD1o7PxuhWDIWk","timestamp":1648492403242},{"file_id":"1UsUqRUa742CvrBeqGtEJLD6PqcDeKcPf","timestamp":1644436192450},{"file_id":"1L7fxLt_ESn4KjgMjbxKQotIddiEFcH7w","timestamp":1643588420054},{"file_id":"1S0J9GiZ3fEkQKjnk2kHaHXjnbJxVhVYT","timestamp":1633715539273},{"file_id":"15Be8c3wLUH9qG6mYyHuK0K84vPVDCooD","timestamp":1630322297284},{"file_id":"1dyUSMhUqiOW4wGYP75YwcsLrE5rspXSQ","timestamp":1630257350538},{"file_id":"1LtOPEFFcRNAkL6JWEGCdevLiGDHbJ3y-","timestamp":1628729514699},{"file_id":"1c6QdqUOAzujzo8B5O_jSL7ahQlRtMQ7f","timestamp":1626811487461},{"file_id":"1xQv1BqMt5xdm3XbtbJaPWM0AGVzCU3TL","timestamp":1626799661875}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}